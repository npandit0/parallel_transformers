{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20359b8e",
   "metadata": {},
   "source": [
    "# Model generation without the kv cache\n",
    "\n",
    "Let's see if we can get the code to work with turning the kv cache off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4313c140-421d-4f1f-a283-3461b8db70ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import jax\n",
    "import equinox as eqx\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util as jtu\n",
    "\n",
    "from functools import partial\n",
    "from equinox._misc import default_floating_dtype\n",
    "from jaxtyping import Array, Float, Scalar\n",
    "from typing import Optional, Tuple, List, NamedTuple\n",
    "\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "672df14d-d052-403a-8b68-b94a6240abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to CPU for torch\n",
    "device  = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7533de2e-9d14-411a-a55a-852cb62646c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tf/ybkfqmld4yb8_yn2xr11sl8m0000gn/T/ipykernel_19202/445611555.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n"
     ]
    }
   ],
   "source": [
    "# Load the model dict, and check if any GPU is used\n",
    "# state_dict = torch.load(\"mistral-7B-v0.1/consolidated.00.pth\")\n",
    "state_dict = torch.load(\n",
    "    \"/Users/xaviergonzalez/Desktop/xavier_folders/stanford/cs229s/mistral_jax/model_files/consolidated.00.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed27c428-fd21-41a1-9a5c-99a966f5a2a3",
   "metadata": {},
   "source": [
    "# 1. Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd26d27d-8e7e-46e9-ba8d-187a8a57a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, model_path: str):\n",
    "        self._model = SentencePieceProcessor(model_file=model_path)\n",
    "\n",
    "    @property\n",
    "    def eos_id(self) -> int:\n",
    "        return self._model.eos_id()\n",
    "\n",
    "    @property\n",
    "    def pad_id(self) -> int:\n",
    "        return self._model.pad_id()\n",
    "\n",
    "    def encode(self, s: str) -> List[int]:\n",
    "        return [self._model.bos_id(), *self._model.encode(s)]\n",
    "\n",
    "    def decode(self, t: List[int]) -> str:\n",
    "        return self._model.decode(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d5aced-2da9-42df-900d-b9d11d5f45fa",
   "metadata": {},
   "source": [
    "# 2. RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "431e5fa1-25dd-401a-abfb-1371f5f1b109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_frequencies(dim, max_pos, theta=10000.0):\n",
    "    inv_freq = 1.0 / (\n",
    "        theta ** (jnp.arange(0, dim, 2, dtype=jnp.float32)[: (dim // 2)] / dim)\n",
    "    )\n",
    "    t = jnp.arange(0, max_pos, dtype=jnp.float32)\n",
    "    freqs = jnp.outer(t, inv_freq)\n",
    "    return jnp.cos(freqs), jnp.sin(freqs)\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(3,))\n",
    "def calculate_rope(x, cos_freq, sin_freq, offset=0):\n",
    "    # x shape  is [seqlen, num_heads, heads_dim]\n",
    "\n",
    "    # Get the sequence length\n",
    "    seqlen = x.shape[0]\n",
    "\n",
    "    # Get the corresponding positional embeddings\n",
    "    sin = sin_freq[offset : offset + seqlen, :]\n",
    "    cos = cos_freq[offset : offset + seqlen, :]\n",
    "\n",
    "    # Positional embeddings are 2D while our input is 3D\n",
    "    # if `num_heads` dimension is present in the inputs.\n",
    "    # We need to add another dimension to our positional embeddings\n",
    "    sin = sin[:, jnp.newaxis, :]\n",
    "    cos = cos[:, jnp.newaxis, :]\n",
    "\n",
    "    # Get the even-odd positions from the inputs\n",
    "    x1 = x[..., 0::2]\n",
    "    x2 = x[..., 1::2]\n",
    "\n",
    "    # Matmul with the rotation matrix\n",
    "    # [cos_nθ, -sin_nθ] [x1]\n",
    "    # [sin_nθ,  cos_nθ] [x2]\n",
    "    # => [x1 * cos_nθ - x2 * sin_nθ, x1 * sin_nθ + x2 * cos_nθ]\n",
    "    pos_embed = jnp.stack([x1 * cos - x2 * sin, x1 * sin + x2 * cos], axis=-1)\n",
    "    pos_embed = jax.lax.collapse(pos_embed, -2)\n",
    "    return pos_embed.astype(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1aa31-614e-4483-8599-c5f0b4623292",
   "metadata": {},
   "source": [
    "# 3. RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "180fbb2d-ce6f-4b1a-a198-e4f37fab93b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(eqx.Module):\n",
    "    eps: float\n",
    "    weight: Float[Array, \"*shape\"]\n",
    "\n",
    "    def __init__(self, dim, eps, dtype=jnp.bfloat16):\n",
    "        dtype = default_floating_dtype if dtype is None else dtype\n",
    "        self.eps = eps\n",
    "        self.weight = jnp.ones(shape=dim, dtype=dtype)\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * jax.lax.rsqrt(jnp.mean(x **2 , keepdims=True) + self.eps)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        output = self._norm(x.astype(jnp.float32)).astype(x.dtype)\n",
    "        return output * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3f9a21-bb3c-45e8-805c-5f8605f72423",
   "metadata": {},
   "source": [
    "# 4. FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efffa74f-556b-4c3e-9f73-e23acfa1da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(eqx.Module):\n",
    "    w1: eqx.nn.Linear\n",
    "    w2: eqx.nn.Linear\n",
    "    w3: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, args, key, dtype=jnp.bfloat16):\n",
    "        dtype = default_floating_dtype if dtype is None else dtype\n",
    "        key1, key2, key3 = jax.random.split(key, 3)\n",
    "\n",
    "        self.w1 = eqx.nn.Linear(args.dim, args.hidden_dim, use_bias=False, key=key1, dtype=dtype)\n",
    "        self.w2 = eqx.nn.Linear(args.hidden_dim, args.dim, use_bias=False, key=key2, dtype=dtype)\n",
    "        self.w3 = eqx.nn.Linear(args.dim, args.hidden_dim, use_bias=False, key=key3, dtype=dtype)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = jax.nn.silu(self.w1(x).astype(jnp.float32)).astype(x.dtype)\n",
    "        return self.w2(h * self.w3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c459d5-b30d-48e5-924b-ea661542e8a2",
   "metadata": {},
   "source": [
    "# 5. Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2048b724-3015-43c8-be5e-0214eb83af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(eqx.Module):\n",
    "    dim: int\n",
    "    n_heads: int\n",
    "    head_dim: int\n",
    "    n_kv_heads: int\n",
    "    kv_repeats: int\n",
    "    sliding_window: int\n",
    "    scale: float\n",
    "    wq: eqx.nn.Linear\n",
    "    wk: eqx.nn.Linear\n",
    "    wv: eqx.nn.Linear\n",
    "    wo: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, args, key, dtype=jnp.bfloat16):\n",
    "        dtype = default_floating_dtype if dtype is None else dtype\n",
    "        key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "\n",
    "        self.n_heads = args.n_heads\n",
    "        self.head_dim = args.head_dim\n",
    "        self.n_kv_heads = args.n_kv_heads\n",
    "        self.dim = args.dim\n",
    "        self.kv_repeats = self.n_heads // self.n_kv_heads\n",
    "        self.sliding_window = args.sliding_window\n",
    "\n",
    "        self.scale = args.head_dim**-0.5\n",
    "\n",
    "        self.wq = eqx.nn.Linear(args.dim, args.n_heads * args.head_dim, use_bias=False, key=key1, dtype=dtype)\n",
    "        self.wk = eqx.nn.Linear(args.dim, args.n_kv_heads * args.head_dim, use_bias=False, key=key2, dtype=dtype)\n",
    "        self.wv = eqx.nn.Linear(args.dim, args.n_kv_heads * args.head_dim, use_bias=False, key=key3, dtype=dtype)\n",
    "        self.wo = eqx.nn.Linear(args.n_heads * args.head_dim, args.dim, use_bias=False, key=key4, dtype=dtype)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(2, 3))\n",
    "    def get_cache_slice(self, x, pos, kv_repeats):\n",
    "        x_slice = x.at[:pos, :, :].get()\n",
    "        x_slice = jnp.repeat(x_slice, kv_repeats, axis=1)\n",
    "        return x_slice\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def compute_qkv(self, x):\n",
    "        seqlen, _ = x.shape\n",
    "\n",
    "        xq = jax.vmap(self.wq)(x)\n",
    "        xk = jax.vmap(self.wk)(x)\n",
    "        xv = jax.vmap(self.wv)(x)\n",
    "\n",
    "        xq = jnp.reshape(xq, (seqlen, self.n_heads, self.head_dim))\n",
    "        xk = jnp.reshape(xk, (seqlen, self.n_kv_heads, self.head_dim))\n",
    "        xv = jnp.reshape(xv, (seqlen, self.n_kv_heads, self.head_dim))\n",
    "        return xq, xk, xv\n",
    "\n",
    "    @jax.jit\n",
    "    def update_cache_values(self, xk, xv, cache_k, cache_v, positions):\n",
    "        cache_k = cache_k.at[positions, ...].set(xk[positions, ...])\n",
    "        cache_v = cache_v.at[positions, ...].set(xv[positions, ...])\n",
    "        return cache_k, cache_v\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def prefill(self, xk, xv):\n",
    "        key = jnp.repeat(xk, self.kv_repeats, axis=1)\n",
    "        value = jnp.repeat(xv, self.kv_repeats, axis=1)\n",
    "        return key, value\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def compute_scores_and_output(self, xq, key, value, mask, seqlen):\n",
    "        query = jnp.transpose(xq, (1, 0, 2))\n",
    "        key = jnp.transpose(key, (1, 0, 2))\n",
    "        value = jnp.transpose(value, (1, 0, 2))\n",
    "\n",
    "        # # # scores : [n_heads, seqlen | 1, seqlen]\n",
    "        scores = jnp.matmul(query, jnp.transpose(key, (0, 2, 1))) * self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            # Mask will of shape [seqlen, seqlen] but our scores\n",
    "            # have shape [num_heads, seqlen, seqlen], hence we need\n",
    "            # to introduce another dimension in the mask\n",
    "            mask = mask[jnp.newaxis, ...]\n",
    "            scores = scores + mask\n",
    "\n",
    "        scores = jax.nn.softmax(scores.astype(jnp.float32), axis=-1).astype(query.dtype)\n",
    "        output = jnp.matmul(scores, value)\n",
    "        output = jnp.reshape(jnp.transpose(output, (1, 0, 2)), (seqlen, -1))\n",
    "        output = jax.vmap(self.wo)(output)\n",
    "        return output\n",
    "\n",
    "    def __call__(self,  x, cos_freq, sin_freq, positions, mask=None, cache_k=None, cache_v=None):\n",
    "        # x shape: [seqlen, embed_dim]\n",
    "        seqlen, _ = x.shape\n",
    "        # 1. Calculate qkv\n",
    "        xq, xk, xv = self.compute_qkv(x)\n",
    "\n",
    "        # 2. Calculate RoPE\n",
    "        xq = calculate_rope(xq, cos_freq, sin_freq, 0)\n",
    "        xk = calculate_rope(xk, cos_freq, sin_freq, 0)\n",
    "\n",
    "        key, value = self.prefill(xk, xv)\n",
    "\n",
    "        # # 3. Update cache\n",
    "        # cache_k, cache_v = self.update_cache_values(xk, xv, cache_k, cache_v, positions)\n",
    "\n",
    "        # # 4. Generation\n",
    "        # if positions.shape[0] > 1:\n",
    "        #     # prefill\n",
    "        #     key, value = self.prefill(xk, xv)\n",
    "        # else:\n",
    "        #     # single-token generation\n",
    "        #     cur_pos = positions[-1].item() + 1\n",
    "        #     key = self.get_cache_slice(cache_k, cur_pos, self.kv_repeats)\n",
    "        #     value = self.get_cache_slice(cache_v, cur_pos, self.kv_repeats)\n",
    "\n",
    "        # 5. Output\n",
    "        output = self.compute_scores_and_output(xq, key, value, mask, seqlen)\n",
    "        # return output, cache_k, cache_v\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129a123d-c4b1-44f0-a6cd-5daf67c63adb",
   "metadata": {},
   "source": [
    "# 6. TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f84cb56f-1a8f-4c28-9f3e-2c180f5e86b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(eqx.Module):\n",
    "    dim: int\n",
    "    n_heads: int\n",
    "    attention: Attention\n",
    "    attention_norm: RMSNorm\n",
    "    feed_forward: FeedForward\n",
    "    ffn_norm: RMSNorm\n",
    "\n",
    "    def __init__(self, args, key, dtype=jnp.bfloat16):\n",
    "        key1, key2 = jax.random.split(key, 2)\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "\n",
    "        self.attention = Attention(args, key=key1, dtype=dtype)\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps, dtype=dtype)\n",
    "\n",
    "        self.feed_forward = FeedForward(args, key=key2, dtype=dtype)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps, dtype=dtype)\n",
    "\n",
    "    # def __call__(self, x, cos_freq, sin_freq, positions, mask, cache_k, cache_v):\n",
    "    def __call__(self, x, cos_freq, sin_freq, positions, mask):\n",
    "        normed_x = jax.vmap(self.attention_norm)(x)\n",
    "        # r, cache_k, cache_v = self.attention(normed_x, cos_freq, sin_freq, positions, mask, cache_k, cache_v)\n",
    "        r = self.attention(\n",
    "            normed_x, cos_freq, sin_freq, positions, mask\n",
    "        )\n",
    "        h = x + r\n",
    "        r = jax.vmap(self.feed_forward)(jax.vmap(self.ffn_norm)(h))\n",
    "        out = h + r\n",
    "        return out\n",
    "        # return out, cache_k, cache_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f9502-010b-42ca-86ac-666e9476ff5c",
   "metadata": {},
   "source": [
    "# 7. Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdec9a64-57c4-4fb5-8e8a-c3ecb4e5bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(eqx.Module):\n",
    "    tok_embeddings: eqx.nn.Embedding\n",
    "    layers: TransformerBlock\n",
    "    norm: RMSNorm\n",
    "    output: eqx.nn.Linear\n",
    "    vocab_size: int\n",
    "    n_layers: int\n",
    "    sliding_window: int\n",
    "\n",
    "    def __init__(self, args, key, dtype=jnp.bfloat16):\n",
    "        self.vocab_size = args.vocab_size\n",
    "        self.n_layers = args.n_layers\n",
    "        self.sliding_window = args.sliding_window\n",
    "        keys = jax.random.split(key, args.n_layers + 2)\n",
    "        embed_key, linear_key, tf_layers_keys = keys[0], keys[1], keys[2:]\n",
    "\n",
    "        self.tok_embeddings = eqx.nn.Embedding(args.vocab_size, args.dim, key=embed_key, dtype=dtype)\n",
    "        self.norm = RMSNorm(dim=args.dim, eps=args.norm_eps, dtype=dtype)\n",
    "        self.output = eqx.nn.Linear(args.dim, args.vocab_size, use_bias=False, key=linear_key, dtype=dtype)\n",
    "        self.layers = [TransformerBlock(args, key=tf_layers_keys[i], dtype=dtype) for i in range(args.n_layers)] \n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def compute_embeddings(self, x):\n",
    "        return jax.vmap(self.tok_embeddings)(x)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def compute_mask(self, seqlen):\n",
    "        t = jnp.full((seqlen, seqlen), dtype=jnp.bfloat16, fill_value=1)\n",
    "        mask = jnp.tril(t, k=0)\n",
    "        # make the mask banded to account for sliding window\n",
    "        mask = jnp.triu(mask, k=-self.sliding_window)\n",
    "        mask = jnp.log(mask)\n",
    "        return mask\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def compute_norm(self, x):\n",
    "        return jax.vmap(self.norm)(x)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def compute_output(self, x):\n",
    "        return jax.vmap(self.output)(x)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(1,))\n",
    "    def update_cache_values(self, idx, cache_k, cache_v, cache_k_updates, cache_v_updates):\n",
    "        cache_k = cache_k.at[idx, :, :, :].set(cache_k_updates)\n",
    "        cache_v = cache_v.at[idx, :, :, :].set(cache_v_updates)\n",
    "        return cache_k, cache_v\n",
    "\n",
    "    # def __call__(self, x, cos_freq, sin_freq, positions, mask, cache_k, cache_v):\n",
    "    #     # x is of shape (seqlen, )\n",
    "    #     h = self.compute_embeddings(x)\n",
    "\n",
    "    #     if x.shape[-1] > 1:\n",
    "    #         seqlen = x.shape[-1]\n",
    "    #         mask = self.compute_mask(seqlen)\n",
    "    #     else:\n",
    "    #         mask = None\n",
    "\n",
    "    #     # the for loop!!!\n",
    "\n",
    "    #     for i, layer in enumerate(self.layers):\n",
    "    #         #pdb.set_trace()\n",
    "    #         # h has shape (len(positions), dim)\n",
    "    #         # cache_ki has shape (sliding_window_len, head_dim, n_kv_heads)\n",
    "    #         h, cache_ki, cache_vi = layer(h, cos_freq, sin_freq, positions, mask, cache_k[i, ...], cache_v[i, ...]) # I think we could get away with creating blank entries for h, cache_ki, and cache_vi\n",
    "    #         # pdb.set_trace()\n",
    "    #         cache_k, cache_v = self.update_cache_values(i, cache_k, cache_v, cache_ki, cache_vi) # I think all this line is doing is plugging in cache_ki and cache_vi in the appropriate palce\n",
    "\n",
    "    #     h = self.compute_norm(h)\n",
    "    #     h = self.compute_output(h).astype(jnp.float32)\n",
    "    #     return h, cache_k, cache_v\n",
    "\n",
    "    def __call__(self, x, cos_freq, sin_freq, positions, mask):\n",
    "        \"\"\"\n",
    "        Edited to do prefilling instead of kv cache\n",
    "        \"\"\"\n",
    "        # x is of shape (seqlen, )\n",
    "        h = self.compute_embeddings(x)\n",
    "\n",
    "        if x.shape[-1] > 1:\n",
    "            seqlen = x.shape[-1]\n",
    "            mask = self.compute_mask(seqlen)\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # h has shape (len(positions), dim)\n",
    "            # cache_ki has shape (sliding_window_len, head_dim, n_kv_heads)\n",
    "            h = layer(h, cos_freq, sin_freq, positions, mask) # h has shape (T,D)\n",
    "            # print(f\"at layer {i}, the shape of the feature is {h.shape}\")\n",
    "\n",
    "        h = self.compute_norm(h)\n",
    "        h = self.compute_output(h).astype(jnp.float32)\n",
    "        return h\n",
    "    \n",
    "    def partial_layers(self, layers, cos_freq, sin_freq, positions, mask):\n",
    "        \"\"\"\n",
    "        Ideally we could use jtu instead...\n",
    "        \"\"\"\n",
    "        def partial_layer(layer):\n",
    "            return partial(layer.__call__, cos_freq=cos_freq, sin_freq=sin_freq, positions=positions, mask=mask)\n",
    "        \n",
    "        return [partial_layer(layer) for layer in layers] # really would prefer not to use list comprehension\n",
    "\n",
    "    def parallel_call(self, x, cos_freq, sin_freq, positions, mask, num_iters=7):\n",
    "        \"\"\"\n",
    "        Should give the same output as call, but using fixed point iterations\n",
    "        \"\"\"\n",
    "        h0 = self.compute_embeddings(x)\n",
    "        T, D = h0.shape\n",
    "\n",
    "        if x.shape[-1] > 1:\n",
    "            seqlen = x.shape[-1]\n",
    "            mask = self.compute_mask(seqlen)\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        # parallel logic\n",
    "        num_layers = len(self.layers)\n",
    "        # pdb.set_trace()\n",
    "        partialed_layers = self.partial_layers(self.layers, cos_freq, sin_freq, positions, mask)\n",
    "        states_guess = [jnp.zeros((T, D)) for _ in range(num_layers)] # we can probably play around with smarter initialization strategies, too\n",
    "        all_states = deer(h0, partialed_layers, states_guess, num_iters)\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        h = self.compute_norm(h)\n",
    "        h = self.compute_output(h).astype(jnp.float32)\n",
    "        return h\n",
    "\n",
    "\n",
    "def deer(x, layers, states_guess, num_iters):\n",
    "    \"\"\"\n",
    "    runs deer (fiddly logic in the rearrange)\n",
    "\n",
    "    Args:\n",
    "      x: (T, d) initial inputs to transformer stack\n",
    "      layers: list of TransformerLayer objects (the functions that propagate information over the stack)\n",
    "      states_guess: list of length num_layers of (T, D) shaped arrays\n",
    "      num_iters: number of iterations to run for\n",
    "    \"\"\"\n",
    "    T, D = x.shape\n",
    "    num_layers = len(layers)\n",
    "\n",
    "    @jax.vmap\n",
    "    def binary_op(q_i, q_j):\n",
    "        \"\"\"Binary operator for parallel scan of linear recurrence. Assumes a full Jacobian matrix A\n",
    "        Args:\n",
    "            q_i: tuple containing J_i and b_i at position i       (P,P), (P,)\n",
    "            q_j: tuple containing J_j and b_j at position j       (P,P), (P,)\n",
    "        Returns:\n",
    "            new element ( A_out, Bu_out )\n",
    "        \"\"\"\n",
    "        A_i, b_i = q_i\n",
    "        A_j, b_j = q_j\n",
    "        return A_j @ A_i, A_j @ b_i + b_j\n",
    "\n",
    "    def step(states, args):\n",
    "        \"\"\"\n",
    "        This step is a single deer iteration (will eventually be sequential scanned)\n",
    "        Args:\n",
    "          states: list of length num_layers of (T, D) shaped arrays\n",
    "          args: None\n",
    "        \"\"\"\n",
    "        states = [x] + states[:-1]  # length num_layers\n",
    "        fs = jnp.array(\n",
    "            jtu.tree_map(lambda x, f: f(x), states, layers)\n",
    "        )  # (num_layers, T,D) arrays, note that we keep states as a list so we can use jtu.tree_map\n",
    "        As = jnp.array(\n",
    "            jtu.tree_map(lambda x, f: jax.jacrev(f)(x), states, layers)\n",
    "        )  # (num_layers, T, D, T, D) tensors\n",
    "        As = As.at[0].set(jnp.zeros((T, D, T, D)))\n",
    "        # need to make the first A equal to zero\n",
    "        states = jnp.array(states)  # (num_layers, T,D)\n",
    "        # do some rearranging\n",
    "        flattened_states = jnp.reshape(states, (num_layers, T * D))  # (num_layers, T*D)\n",
    "        flattened_As = jnp.reshape(\n",
    "            As, (num_layers, T * D, T * D)\n",
    "        )  # (num_layers, T*D, T*D)\n",
    "        flattened_fs = jnp.reshape(fs, (num_layers, T * D))  # (num_layers, T*D)\n",
    "        bs = flattened_fs - jnp.einsum(\n",
    "            \"tij,tj->ti\", flattened_As, flattened_states\n",
    "        )  # (num_layers, T*D)\n",
    "\n",
    "        # finally ready to evaluate linearized dynamics (in parallel)\n",
    "        _, new_states = jax.lax.associative_scan(\n",
    "            binary_op, (flattened_As, bs)\n",
    "        )  # parallel operation\n",
    "        new_states = jnp.nan_to_num(new_states)  # zero out nans, (num_layers, T*D)\n",
    "        new_states = jnp.reshape(new_states, (num_layers, T, D))  # (num_layers, T, D)\n",
    "        return list(new_states), new_states\n",
    "\n",
    "    print(\"starting deer outer loop\")\n",
    "    _, iter_hist = jax.lax.scan(\n",
    "        step, states_guess, None, length=num_iters\n",
    "    )  # state_iters will show all the intermediate traces\n",
    "    return iter_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5a7c63f-4be6-4bad-b6f5-b77c77bcc56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelArgs(NamedTuple):\n",
    "    dim: int\n",
    "    n_layers: int\n",
    "    n_heads: int\n",
    "    n_kv_heads: int\n",
    "    head_dim: int\n",
    "    hidden_dim: int\n",
    "    vocab_size: int\n",
    "    sliding_window: int\n",
    "    norm_eps: float\n",
    "    max_batch_size: int = 1\n",
    "\n",
    "# maybe we could do a proto_params.json, and just not load in the weights\n",
    "with open(\n",
    "    \"/Users/xaviergonzalez/Desktop/xavier_folders/stanford/cs229s/mistral_jax/model_files/params.json\",\n",
    "    \"r\",\n",
    ") as f:\n",
    "    # with open('./mistral-7B-v0.1/params.json', 'r') as f:\n",
    "    args = ModelArgs(**json.loads(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16a2d2cf-40f6-48af-a70f-37847532e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def port_weights_from_torch(torch_weights, eqx_model):\n",
    "    def load_weights(path, leaf):\n",
    "        path_pieces = []\n",
    "        for path_elem in path:\n",
    "            if isinstance(path_elem, jax.tree_util.GetAttrKey):\n",
    "                 path_pieces.append(path_elem.name)\n",
    "            elif isinstance(path_elem, jax.tree_util.SequenceKey):\n",
    "                 path_pieces.append(str(path_elem.idx))\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported path type {type(path_elem)}\")\n",
    "\n",
    "        path_pieces = \".\".join(path_pieces)\n",
    "        \n",
    "        if \"weight\" in path_pieces:\n",
    "            weight = torch_weights[path_pieces]\n",
    "            weight = jnp.asarray(weight.float().numpy(), dtype=jnp.bfloat16)\n",
    "            assert weight.shape == leaf.shape\n",
    "            assert weight.dtype == leaf.dtype\n",
    "            return weight\n",
    "        else:\n",
    "            print(f\"Weights not ported for: {path_pieces}\")\n",
    "            return leaf\n",
    "\n",
    "    return jax.tree_util.tree_map_with_path(load_weights, eqx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67530c78-98a9-4f84-aa4b-af0819b5f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(args, key=jax.random.PRNGKey(1), dtype=jnp.bfloat16) # sets architecutre\n",
    "# model = port_weights_from_torch(state_dict, model) # fills with pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da969f6-eb91-4df5-9976-1db480914a26",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# cache_k = jnp.zeros((args.max_batch_size, args.n_layers, args.sliding_window, args.n_kv_heads, args.head_dim), dtype=jnp.bfloat16)\n",
    "# cache_v = jnp.zeros((args.max_batch_size, args.n_layers, args.sliding_window, args.n_kv_heads, args.head_dim), dtype=jnp.bfloat16)\n",
    "cos_freq, sin_freq = precompute_frequencies(args.head_dim, 128000)\n",
    "vmapped = jax.vmap(partial(model.parallel_call, num_iters=2), in_axes=(0, None, None, None, None)) # vmapped is the name of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d96e5-f58d-4ea4-b878-92abcfcb85e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we make it work on this example withou the kv cache?\n",
    "# set random seeds, try to get the same output with regular __call__ vs parallel_call\n",
    "fake_pos = jnp.array([0, 1, 2, 3, 4], dtype=jnp.int32)\n",
    "fake_inp = jnp.asarray([[1,  832,  349,  265, 1369]], dtype=jnp.int32)\n",
    "fake_mask = None\n",
    "\n",
    "# warmup\n",
    "logits = vmapped(fake_inp, cos_freq[fake_pos], sin_freq[fake_pos], fake_pos, fake_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61e48f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/var/folders/tf/ybkfqmld4yb8_yn2xr11sl8m0000gn/T/ipykernel_3900/2336997464.py\u001b[0m(98)\u001b[0;36mpartial_layer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     96 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mpartial_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     97 \u001b[0;31m        \u001b[0;32mdef\u001b[0m \u001b[0mpartial_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 98 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcos_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msin_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     99 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    100 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mjtu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "4096\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b4f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 32000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b45669",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_inp_flat = jnp.asarray([1, 832, 349, 265, 1369], dtype=jnp.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2ae933",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = Attention(args, key=jax.random.PRNGKey(1), dtype=jnp.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc5c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128000, 64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_freq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac0483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4096)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compute_embeddings(fake_inp_flat).shape # (T, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f2421f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc248cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.compute_embeddings(fake_inp_flat)\n",
    "xq, xk, xv = attn.compute_qkv(x)\n",
    "key, value = attn.prefill(xk, xv)\n",
    "output = attn.compute_scores_and_output(xq, key, value, fake_mask, x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406da556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4096)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e23a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4096)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949a4e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 8, 128)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a71fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 32, 128)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe1f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 32, 128)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1392db12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 32, 128)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e8e3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 32, 128)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq.shape # (T, n_heads, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe4f380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 8, 128)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xk.shape # (T, n_kv_heads, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167bed62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 8, 128)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xv.shape # (T, n_kv_heads, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5c6b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn.compute_scores_and_output(xq, xk, xv, fake_mask, 5).shape # (T, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900a9b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_pos.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5374a93",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'at'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_inp_flat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos_freq\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin_freq\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_pos\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 96\u001b[0m, in \u001b[0;36mAttention.__call__\u001b[0;34m(self, x, cos_freq, sin_freq, positions, mask, cache_k, cache_v)\u001b[0m\n\u001b[1;32m     93\u001b[0m xk \u001b[38;5;241m=\u001b[39m calculate_rope(xk, cos_freq, sin_freq, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# 3. Update cache\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m cache_k, cache_v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_cache_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# 4. Generation\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m positions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# prefill\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[8], line 53\u001b[0m, in \u001b[0;36mAttention.update_cache_values\u001b[0;34m(self, xk, xv, cache_k, cache_v, positions)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;129m@jax\u001b[39m\u001b[38;5;241m.\u001b[39mjit\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_cache_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, xk, xv, cache_k, cache_v, positions):\n\u001b[0;32m---> 53\u001b[0m     cache_k \u001b[38;5;241m=\u001b[39m \u001b[43mcache_k\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat\u001b[49m[positions, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\u001b[38;5;241m.\u001b[39mset(xk[positions, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m])\n\u001b[1;32m     54\u001b[0m     cache_v \u001b[38;5;241m=\u001b[39m cache_v\u001b[38;5;241m.\u001b[39mat[positions, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\u001b[38;5;241m.\u001b[39mset(xv[positions, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m])\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cache_k, cache_v\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'at'"
     ]
    }
   ],
   "source": [
    "attn(model.compute_embeddings(fake_inp_flat), cos_freq[:5], sin_freq[:5], fake_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3410598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc44e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 4096)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.vmap(model.compute_embeddings)(fake_inp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c401172d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 32000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape # (batch, T, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a48e48",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`eqx.nn.Embedding()(x)` should be called with a scalar index `x`. Use `jax.vmap` if you would like to index with multiple values.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfake_inp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcos_freq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfake_pos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msin_freq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfake_pos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfake_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfake_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 53\u001b[0m, in \u001b[0;36mTransformer.__call__\u001b[0;34m(self, x, cos_freq, sin_freq, positions, mask, cache_k, cache_v)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, cos_freq, sin_freq, positions, mask, cache_k, cache_v):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# x is of shape (seqlen, )\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     56\u001b[0m         seqlen \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mistral-eqx/lib/python3.12/site-packages/equinox/_module.py:1189\u001b[0m, in \u001b[0;36mPartial.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the wrapped `self.func`.\u001b[39;00m\n\u001b[1;32m   1178\u001b[0m \n\u001b[1;32m   1179\u001b[0m \u001b[38;5;124;03m    **Arguments:**\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;124;03m    The result of the wrapped function.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeywords\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 16 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m, in \u001b[0;36mTransformer.compute_embeddings\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;129m@eqx\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_jit\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtok_embeddings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mistral-eqx/lib/python3.12/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mistral-eqx/lib/python3.12/site-packages/equinox/nn/_embedding.py:101\u001b[0m, in \u001b[0;36mEmbedding.__call__\u001b[0;34m(self, x, key)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight[x]\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`eqx.nn.Embedding()(x)` should be called with a scalar index `x`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `jax.vmap` if you would like to index with multiple values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: `eqx.nn.Embedding()(x)` should be called with a scalar index `x`. Use `jax.vmap` if you would like to index with multiple values."
     ]
    }
   ],
   "source": [
    "model(\n",
    "    fake_inp,\n",
    "    cos_freq[fake_pos],\n",
    "    sin_freq[fake_pos],\n",
    "    fake_pos,\n",
    "    fake_mask,\n",
    "    cache_k,\n",
    "    cache_v,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c5dec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/xaviergonzalez/opt/anaconda3/envs/mistral-eqx/lib/python3.12/site-packages/equinox/nn/_embedding.py\u001b[0m(101)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     99 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    100 \u001b[0;31m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 101 \u001b[0;31m            raise ValueError(\n",
      "\u001b[0m\u001b[0;32m    102 \u001b[0;31m                \u001b[0;34m\"`eqx.nn.Embedding()(x)` should be called with a scalar index `x`. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    103 \u001b[0;31m                \u001b[0;34m\"Use `jax.vmap` if you would like to index with multiple values.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/Users/xaviergonzalez/opt/anaconda3/envs/mistral-eqx/lib/python3.12/contextlib.py\u001b[0m(81)\u001b[0;36minner\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     79 \u001b[0;31m        \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     80 \u001b[0;31m            \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 81 \u001b[0;31m                \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     82 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     83 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/var/folders/tf/ybkfqmld4yb8_yn2xr11sl8m0000gn/T/ipykernel_2338/467368851.py\u001b[0m(24)\u001b[0;36mcompute_embeddings\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     22 \u001b[0;31m    \u001b[0;34m@\u001b[0m\u001b[0meqx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_jit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     23 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mcompute_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 24 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     25 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     26 \u001b[0;31m    \u001b[0;34m@\u001b[0m\u001b[0meqx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_jit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;31m    [... skipped 3 hidden frame(s)]\u001b[0m\n",
      "\n",
      "(1, 5)\n",
      "*** ValueError: `eqx.nn.Embedding()(x)` should be called with a scalar index `x`. Use `jax.vmap` if you would like to index with multiple values.\n",
      "*** ValueError: `eqx.nn.Embedding()(x)` should be called with a scalar index `x`. Use `jax.vmap` if you would like to index with multiple values.\n",
      "*** jax.errors.UnexpectedTracerError: Encountered an unexpected tracer. A function transformed by JAX had a side effect, allowing for a reference to an intermediate value with type int32[1,5] wrapped in a DynamicJaxprTracer to escape the scope of the transformation.\n",
      "JAX transformations require that functions explicitly return their outputs, and disallow saving intermediate values to global state.\n",
      "The function being traced when the value leaked was compute_embeddings at /Users/xaviergonzalez/opt/anaconda3/envs/mistral-eqx/lib/python3.12/site-packages/equinox/_jit.py:37 traced for jit.\n",
      "------------------------------\n",
      "The leaked intermediate value was created on line /var/folders/tf/ybkfqmld4yb8_yn2xr11sl8m0000gn/T/ipykernel_2338/467368851.py:53:12 (Transformer.__call__). \n",
      "------------------------------\n",
      "When the value was created, the final 5 stack frames (most recent last) excluding JAX-internal frames were:\n",
      "------------------------------\n",
      "<frozen runpy>:198:11 (_run_module_as_main)\n",
      "<frozen runpy>:88:4 (_run_code)\n",
      "/var/folders/tf/ybkfqmld4yb8_yn2xr11sl8m0000gn/T/ipykernel_2338/1221483641.py:1 (<module>)\n",
      "/var/folders/tf/ybkfqmld4yb8_yn2xr11sl8m0000gn/T/ipykernel_2338/467368851.py:53:12 (Transformer.__call__)\n",
      "------------------------------\n",
      "\n",
      "To catch the leak earlier, try setting the environment variable JAX_CHECK_TRACER_LEAKS or using the `jax.checking_leaks` context manager.\n",
      "See https://jax.readthedocs.io/en/latest/errors.html#jax.errors.UnexpectedTracerError\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e223d912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral-eqx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
