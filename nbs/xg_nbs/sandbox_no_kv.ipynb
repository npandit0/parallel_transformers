{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20359b8e",
   "metadata": {},
   "source": [
    "# Model generation without the kv cache\n",
    "\n",
    "Let's see if we can get the code to work with turning the kv cache off.\n",
    "\n",
    "How the eigenvaleus be so large if you are using layer norm? In fact, the evals are large BECAUSE we are using RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4313c140-421d-4f1f-a283-3461b8db70ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import jax\n",
    "import equinox as eqx\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util as jtu\n",
    "\n",
    "from functools import partial\n",
    "from equinox._misc import default_floating_dtype\n",
    "from jaxtyping import Array, Float, Scalar\n",
    "from typing import Optional, Tuple, List, NamedTuple\n",
    "\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9a7657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_debug_nans\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "672df14d-d052-403a-8b68-b94a6240abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to CPU for torch\n",
    "device  = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7533de2e-9d14-411a-a55a-852cb62646c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tf/ybkfqmld4yb8_yn2xr11sl8m0000gn/T/ipykernel_3703/2163712912.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n"
     ]
    }
   ],
   "source": [
    "# Load the model dict, and check if any GPU is used\n",
    "# state_dict = torch.load(\"mistral-7B-v0.1/consolidated.00.pth\")\n",
    "# why is this so much faster on our computer??\n",
    "state_dict = torch.load(\n",
    "    \"/Users/xaviergonzalez/Desktop/xavier_folders/stanford/cs229s/mistral_jax/model_files/consolidated.00.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed27c428-fd21-41a1-9a5c-99a966f5a2a3",
   "metadata": {},
   "source": [
    "# 1. Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd26d27d-8e7e-46e9-ba8d-187a8a57a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, model_path: str):\n",
    "        self._model = SentencePieceProcessor(model_file=model_path)\n",
    "\n",
    "    @property\n",
    "    def eos_id(self) -> int:\n",
    "        return self._model.eos_id()\n",
    "\n",
    "    @property\n",
    "    def pad_id(self) -> int:\n",
    "        return self._model.pad_id()\n",
    "\n",
    "    def encode(self, s: str) -> List[int]:\n",
    "        return [self._model.bos_id(), *self._model.encode(s)]\n",
    "\n",
    "    def decode(self, t: List[int]) -> str:\n",
    "        return self._model.decode(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d5aced-2da9-42df-900d-b9d11d5f45fa",
   "metadata": {},
   "source": [
    "# 2. RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "431e5fa1-25dd-401a-abfb-1371f5f1b109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_frequencies(dim, max_pos, theta=10000.0):\n",
    "    inv_freq = 1.0 / (\n",
    "        theta ** (jnp.arange(0, dim, 2, dtype=jnp.float32)[: (dim // 2)] / dim)\n",
    "    )\n",
    "    t = jnp.arange(0, max_pos, dtype=jnp.float32)\n",
    "    freqs = jnp.outer(t, inv_freq)\n",
    "    return jnp.cos(freqs), jnp.sin(freqs)\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(3,))\n",
    "def calculate_rope(x, cos_freq, sin_freq, offset=0):\n",
    "    # x shape  is [seqlen, num_heads, heads_dim]\n",
    "\n",
    "    # Get the sequence length\n",
    "    seqlen = x.shape[0]\n",
    "\n",
    "    # Get the corresponding positional embeddings\n",
    "    sin = sin_freq[offset : offset + seqlen, :]\n",
    "    cos = cos_freq[offset : offset + seqlen, :]\n",
    "\n",
    "    # Positional embeddings are 2D while our input is 3D\n",
    "    # if `num_heads` dimension is present in the inputs.\n",
    "    # We need to add another dimension to our positional embeddings\n",
    "    sin = sin[:, jnp.newaxis, :]\n",
    "    cos = cos[:, jnp.newaxis, :]\n",
    "\n",
    "    # Get the even-odd positions from the inputs\n",
    "    x1 = x[..., 0::2]\n",
    "    x2 = x[..., 1::2]\n",
    "\n",
    "    # Matmul with the rotation matrix\n",
    "    # [cos_nθ, -sin_nθ] [x1]\n",
    "    # [sin_nθ,  cos_nθ] [x2]\n",
    "    # => [x1 * cos_nθ - x2 * sin_nθ, x1 * sin_nθ + x2 * cos_nθ]\n",
    "    pos_embed = jnp.stack([x1 * cos - x2 * sin, x1 * sin + x2 * cos], axis=-1)\n",
    "    pos_embed = jax.lax.collapse(pos_embed, -2)\n",
    "    return pos_embed.astype(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1aa31-614e-4483-8599-c5f0b4623292",
   "metadata": {},
   "source": [
    "# 3. RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "180fbb2d-ce6f-4b1a-a198-e4f37fab93b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(eqx.Module):\n",
    "    \"\"\"\n",
    "    Make the root mean square of the output to be 1.\n",
    "    \"\"\"\n",
    "    eps: float\n",
    "    weight: Float[Array, \"*shape\"]\n",
    "\n",
    "    def __init__(self, dim, eps, dtype=jnp.bfloat16):\n",
    "        dtype = default_floating_dtype if dtype is None else dtype\n",
    "        self.eps = eps\n",
    "        self.weight = jnp.ones(shape=dim, dtype=dtype)\n",
    "\n",
    "    def rmsnorm(self, x):\n",
    "        return jnp.sqrt(jnp.mean(x**2, keepdims=True) + self.eps)\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * jax.lax.rsqrt(jnp.mean(x **2 , keepdims=True) + self.eps)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        output = self._norm(x.astype(jnp.float32)).astype(x.dtype)\n",
    "        return output * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3f9a21-bb3c-45e8-805c-5f8605f72423",
   "metadata": {},
   "source": [
    "# 4. FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efffa74f-556b-4c3e-9f73-e23acfa1da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(eqx.Module):\n",
    "    w1: eqx.nn.Linear\n",
    "    w2: eqx.nn.Linear\n",
    "    w3: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, args, key, dtype=jnp.bfloat16):\n",
    "        dtype = default_floating_dtype if dtype is None else dtype\n",
    "        key1, key2, key3 = jax.random.split(key, 3)\n",
    "\n",
    "        self.w1 = eqx.nn.Linear(args.dim, args.hidden_dim, use_bias=False, key=key1, dtype=dtype)\n",
    "        self.w2 = eqx.nn.Linear(args.hidden_dim, args.dim, use_bias=False, key=key2, dtype=dtype)\n",
    "        self.w3 = eqx.nn.Linear(args.dim, args.hidden_dim, use_bias=False, key=key3, dtype=dtype)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = jax.nn.silu(self.w1(x).astype(jnp.float32)).astype(x.dtype)\n",
    "        return self.w2(h * self.w3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c459d5-b30d-48e5-924b-ea661542e8a2",
   "metadata": {},
   "source": [
    "# 5. Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2048b724-3015-43c8-be5e-0214eb83af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(eqx.Module):\n",
    "    dim: int\n",
    "    n_heads: int\n",
    "    head_dim: int\n",
    "    n_kv_heads: int\n",
    "    kv_repeats: int\n",
    "    sliding_window: int\n",
    "    scale: float\n",
    "    wq: eqx.nn.Linear\n",
    "    wk: eqx.nn.Linear\n",
    "    wv: eqx.nn.Linear\n",
    "    wo: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, args, key, dtype=jnp.bfloat16):\n",
    "        dtype = default_floating_dtype if dtype is None else dtype\n",
    "        key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "\n",
    "        self.n_heads = args.n_heads\n",
    "        self.head_dim = args.head_dim\n",
    "        self.n_kv_heads = args.n_kv_heads\n",
    "        self.dim = args.dim\n",
    "        self.kv_repeats = self.n_heads // self.n_kv_heads\n",
    "        self.sliding_window = args.sliding_window\n",
    "\n",
    "        self.scale = args.head_dim**-0.5\n",
    "\n",
    "        self.wq = eqx.nn.Linear(args.dim, args.n_heads * args.head_dim, use_bias=False, key=key1, dtype=dtype)\n",
    "        self.wk = eqx.nn.Linear(args.dim, args.n_kv_heads * args.head_dim, use_bias=False, key=key2, dtype=dtype)\n",
    "        self.wv = eqx.nn.Linear(args.dim, args.n_kv_heads * args.head_dim, use_bias=False, key=key3, dtype=dtype)\n",
    "        self.wo = eqx.nn.Linear(args.n_heads * args.head_dim, args.dim, use_bias=False, key=key4, dtype=dtype)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(2, 3))\n",
    "    def get_cache_slice(self, x, pos, kv_repeats):\n",
    "        x_slice = x.at[:pos, :, :].get()\n",
    "        x_slice = jnp.repeat(x_slice, kv_repeats, axis=1)\n",
    "        return x_slice\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def compute_qkv(self, x):\n",
    "        seqlen, _ = x.shape\n",
    "\n",
    "        xq = jax.vmap(self.wq)(x)\n",
    "        xk = jax.vmap(self.wk)(x)\n",
    "        xv = jax.vmap(self.wv)(x)\n",
    "\n",
    "        xq = jnp.reshape(xq, (seqlen, self.n_heads, self.head_dim))\n",
    "        xk = jnp.reshape(xk, (seqlen, self.n_kv_heads, self.head_dim))\n",
    "        xv = jnp.reshape(xv, (seqlen, self.n_kv_heads, self.head_dim))\n",
    "        return xq, xk, xv\n",
    "\n",
    "    @jax.jit\n",
    "    def update_cache_values(self, xk, xv, cache_k, cache_v, positions):\n",
    "        cache_k = cache_k.at[positions, ...].set(xk[positions, ...])\n",
    "        cache_v = cache_v.at[positions, ...].set(xv[positions, ...])\n",
    "        return cache_k, cache_v\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def prefill(self, xk, xv):\n",
    "        key = jnp.repeat(xk, self.kv_repeats, axis=1)\n",
    "        value = jnp.repeat(xv, self.kv_repeats, axis=1)\n",
    "        return key, value\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def compute_scores_and_output(self, xq, key, value, mask, seqlen):\n",
    "        query = jnp.transpose(xq, (1, 0, 2))\n",
    "        key = jnp.transpose(key, (1, 0, 2))\n",
    "        value = jnp.transpose(value, (1, 0, 2))\n",
    "\n",
    "        # # # scores : [n_heads, seqlen | 1, seqlen]\n",
    "        scores = jnp.matmul(query, jnp.transpose(key, (0, 2, 1))) * self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            # Mask will of shape [seqlen, seqlen] but our scores\n",
    "            # have shape [num_heads, seqlen, seqlen], hence we need\n",
    "            # to introduce another dimension in the mask\n",
    "            mask = mask[jnp.newaxis, ...]\n",
    "            scores = scores + mask\n",
    "\n",
    "        scores = jax.nn.softmax(scores.astype(jnp.float32), axis=-1).astype(query.dtype)\n",
    "        output = jnp.matmul(scores, value)\n",
    "        output = jnp.reshape(jnp.transpose(output, (1, 0, 2)), (seqlen, -1))\n",
    "        output = jax.vmap(self.wo)(output)\n",
    "        return output\n",
    "\n",
    "    def __call__(self,  x, cos_freq, sin_freq, positions, mask=None, cache_k=None, cache_v=None):\n",
    "        # x shape: [seqlen, embed_dim]\n",
    "        seqlen, _ = x.shape\n",
    "        # 1. Calculate qkv\n",
    "        xq, xk, xv = self.compute_qkv(x)\n",
    "\n",
    "        # 2. Calculate RoPE\n",
    "        xq = calculate_rope(xq, cos_freq, sin_freq, 0)\n",
    "        xk = calculate_rope(xk, cos_freq, sin_freq, 0)\n",
    "\n",
    "        key, value = self.prefill(xk, xv)\n",
    "\n",
    "        # # 3. Update cache\n",
    "        # cache_k, cache_v = self.update_cache_values(xk, xv, cache_k, cache_v, positions)\n",
    "\n",
    "        # # 4. Generation\n",
    "        # if positions.shape[0] > 1:\n",
    "        #     # prefill\n",
    "        #     key, value = self.prefill(xk, xv)\n",
    "        # else:\n",
    "        #     # single-token generation\n",
    "        #     cur_pos = positions[-1].item() + 1\n",
    "        #     key = self.get_cache_slice(cache_k, cur_pos, self.kv_repeats)\n",
    "        #     value = self.get_cache_slice(cache_v, cur_pos, self.kv_repeats)\n",
    "\n",
    "        # 5. Output\n",
    "        output = self.compute_scores_and_output(xq, key, value, mask, seqlen)\n",
    "        # return output, cache_k, cache_v\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129a123d-c4b1-44f0-a6cd-5daf67c63adb",
   "metadata": {},
   "source": [
    "# 6. TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f84cb56f-1a8f-4c28-9f3e-2c180f5e86b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(eqx.Module):\n",
    "    dim: int\n",
    "    n_heads: int\n",
    "    attention: Attention\n",
    "    attention_norm: RMSNorm\n",
    "    feed_forward: FeedForward\n",
    "    ffn_norm: RMSNorm\n",
    "\n",
    "    def __init__(self, args, key, dtype=jnp.bfloat16):\n",
    "        key1, key2 = jax.random.split(key, 2)\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "\n",
    "        self.attention = Attention(args, key=key1, dtype=dtype)\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps, dtype=dtype)\n",
    "\n",
    "        self.feed_forward = FeedForward(args, key=key2, dtype=dtype)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps, dtype=dtype)\n",
    "\n",
    "    # def __call__(self, x, cos_freq, sin_freq, positions, mask, cache_k, cache_v):\n",
    "    def __call__(self, x, cos_freq, sin_freq, positions, mask):\n",
    "        normed_x = jax.vmap(self.attention_norm)(x)\n",
    "        # r, cache_k, cache_v = self.attention(normed_x, cos_freq, sin_freq, positions, mask, cache_k, cache_v)\n",
    "        r = self.attention(\n",
    "            normed_x, cos_freq, sin_freq, positions, mask\n",
    "        )\n",
    "        h = x + r\n",
    "        r = jax.vmap(self.feed_forward)(jax.vmap(self.ffn_norm)(h))\n",
    "        out = h + r\n",
    "        return out\n",
    "        # return out, cache_k, cache_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f9502-010b-42ca-86ac-666e9476ff5c",
   "metadata": {},
   "source": [
    "# 7. Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdec9a64-57c4-4fb5-8e8a-c3ecb4e5bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(eqx.Module):\n",
    "    tok_embeddings: eqx.nn.Embedding\n",
    "    layers: TransformerBlock\n",
    "    norm: RMSNorm\n",
    "    output: eqx.nn.Linear\n",
    "    vocab_size: int\n",
    "    n_layers: int\n",
    "    sliding_window: int\n",
    "\n",
    "    def __init__(self, args, key, dtype=jnp.bfloat16):\n",
    "        self.vocab_size = args.vocab_size\n",
    "        self.n_layers = args.n_layers\n",
    "        self.sliding_window = args.sliding_window\n",
    "        keys = jax.random.split(key, args.n_layers + 2)\n",
    "        embed_key, linear_key, tf_layers_keys = keys[0], keys[1], keys[2:]\n",
    "\n",
    "        self.tok_embeddings = eqx.nn.Embedding(args.vocab_size, args.dim, key=embed_key, dtype=dtype)\n",
    "        self.norm = RMSNorm(dim=args.dim, eps=args.norm_eps, dtype=dtype)\n",
    "        self.output = eqx.nn.Linear(args.dim, args.vocab_size, use_bias=False, key=linear_key, dtype=dtype)\n",
    "        self.layers = [TransformerBlock(args, key=tf_layers_keys[i], dtype=dtype) for i in range(args.n_layers)] \n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def compute_embeddings(self, x):\n",
    "        return jax.vmap(self.tok_embeddings)(x)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def compute_mask(self, seqlen):\n",
    "        t = jnp.full((seqlen, seqlen), dtype=jnp.bfloat16, fill_value=1)\n",
    "        mask = jnp.tril(t, k=0)\n",
    "        # make the mask banded to account for sliding window\n",
    "        mask = jnp.triu(mask, k=-self.sliding_window)\n",
    "        mask = jnp.log(mask)\n",
    "        return mask\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def compute_norm(self, x):\n",
    "        return jax.vmap(self.norm)(x)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def compute_output(self, x):\n",
    "        return jax.vmap(self.output)(x)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(1,))\n",
    "    def update_cache_values(self, idx, cache_k, cache_v, cache_k_updates, cache_v_updates):\n",
    "        cache_k = cache_k.at[idx, :, :, :].set(cache_k_updates)\n",
    "        cache_v = cache_v.at[idx, :, :, :].set(cache_v_updates)\n",
    "        return cache_k, cache_v\n",
    "\n",
    "    # def __call__(self, x, cos_freq, sin_freq, positions, mask, cache_k, cache_v):\n",
    "    #     # x is of shape (seqlen, )\n",
    "    #     h = self.compute_embeddings(x)\n",
    "\n",
    "    #     if x.shape[-1] > 1:\n",
    "    #         seqlen = x.shape[-1]\n",
    "    #         mask = self.compute_mask(seqlen)\n",
    "    #     else:\n",
    "    #         mask = None\n",
    "\n",
    "    #     # the for loop!!!\n",
    "\n",
    "    #     for i, layer in enumerate(self.layers):\n",
    "    #         #pdb.set_trace()\n",
    "    #         # h has shape (len(positions), dim)\n",
    "    #         # cache_ki has shape (sliding_window_len, head_dim, n_kv_heads)\n",
    "    #         h, cache_ki, cache_vi = layer(h, cos_freq, sin_freq, positions, mask, cache_k[i, ...], cache_v[i, ...]) # I think we could get away with creating blank entries for h, cache_ki, and cache_vi\n",
    "    #         # pdb.set_trace()\n",
    "    #         cache_k, cache_v = self.update_cache_values(i, cache_k, cache_v, cache_ki, cache_vi) # I think all this line is doing is plugging in cache_ki and cache_vi in the appropriate palce\n",
    "\n",
    "    #     h = self.compute_norm(h)\n",
    "    #     h = self.compute_output(h).astype(jnp.float32)\n",
    "    #     return h, cache_k, cache_v\n",
    "\n",
    "    def __call__(self, x, cos_freq, sin_freq, positions, mask):\n",
    "        \"\"\"\n",
    "        Edited to do prefilling instead of kv cache\n",
    "        \"\"\"\n",
    "        # x is of shape (seqlen, )\n",
    "        h = self.compute_embeddings(x)\n",
    "\n",
    "        if x.shape[-1] > 1:\n",
    "            seqlen = x.shape[-1]\n",
    "            mask = self.compute_mask(seqlen)\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        all_states = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # h has shape (len(positions), dim)\n",
    "            # cache_ki has shape (sliding_window_len, head_dim, n_kv_heads)\n",
    "            h = layer(h, cos_freq, sin_freq, positions, mask) # h has shape (T,D)\n",
    "            all_states.append(h)\n",
    "            # print(f\"at layer {i}, the shape of the feature is {h.shape}\")\n",
    "\n",
    "        # h = self.compute_norm(h)\n",
    "        # h = self.compute_output(h).astype(jnp.float32)\n",
    "        return jnp.array(all_states)\n",
    "\n",
    "    def partial_layers(self, layers, cos_freq, sin_freq, positions, mask):\n",
    "        \"\"\"\n",
    "        Ideally we could use jtu instead...\n",
    "        \"\"\"\n",
    "        def partial_layer(layer):\n",
    "            return partial(layer.__call__, cos_freq=cos_freq, sin_freq=sin_freq, positions=positions, mask=mask)\n",
    "\n",
    "        return [partial_layer(layer) for layer in layers] # really would prefer not to use list comprehension\n",
    "\n",
    "    def parallel_call(self, x, cos_freq, sin_freq, positions, mask, num_iters=7):\n",
    "        \"\"\"\n",
    "        Should give the same output as call, but using fixed point iterations\n",
    "        \"\"\"\n",
    "        h0 = self.compute_embeddings(x)\n",
    "        T, D = h0.shape\n",
    "\n",
    "        if x.shape[-1] > 1:\n",
    "            seqlen = x.shape[-1]\n",
    "            mask = self.compute_mask(seqlen)\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        # parallel logic\n",
    "        num_layers = len(self.layers)\n",
    "        # pdb.set_trace()\n",
    "        partialed_layers = self.partial_layers(self.layers, cos_freq, sin_freq, positions, mask)\n",
    "        states_guess = [\n",
    "            jnp.ones((T, D)) for _ in range(num_layers)\n",
    "        ]  # make sure to stay near rms norm equal to 1\n",
    "        # states_guess = [jnp.zeros((T, D)) for _ in range(num_layers)] # never do this when using rms norm, grads will explode\n",
    "        # calls out to deer\n",
    "        all_states = deer(h0, partialed_layers, states_guess, num_iters) # (batch_size, num_iters, num_layers, T, D)\n",
    "\n",
    "        return jnp.array(all_states)\n",
    "        # h = all_states[-1][-1]\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        # h = self.compute_norm(h)\n",
    "        # h = self.compute_output(h).astype(jnp.float32)\n",
    "        # return h\n",
    "\n",
    "\n",
    "def deer(x, layers, states_guess, num_iters, k =1):\n",
    "    \"\"\"\n",
    "    runs deer (fiddly logic in the rearrange)\n",
    "\n",
    "    Args:\n",
    "      x: (T, d) initial inputs to transformer stack\n",
    "      layers: list of TransformerLayer objects (the functions that propagate information over the stack)\n",
    "      states_guess: list of length num_layers of (T, D) shaped arrays; this is the initial guess for the states. don't make them all zero!\n",
    "      num_iters: number of iterations to run for\n",
    "      k: damping factor\n",
    "    \"\"\"\n",
    "    T, D = x.shape\n",
    "    num_layers = len(layers)\n",
    "\n",
    "    @jax.vmap\n",
    "    def binary_op(q_i, q_j):\n",
    "        \"\"\"Binary operator for parallel scan of linear recurrence. Assumes a full Jacobian matrix A\n",
    "        Args:\n",
    "            q_i: tuple containing J_i and b_i at position i       (P,P), (P,)\n",
    "            q_j: tuple containing J_j and b_j at position j       (P,P), (P,)\n",
    "        Returns:\n",
    "            new element ( A_out, Bu_out )\n",
    "        \"\"\"\n",
    "        A_i, b_i = q_i\n",
    "        A_j, b_j = q_j\n",
    "        return A_j @ A_i, A_j @ b_i + b_j\n",
    "\n",
    "    def step(states, args):\n",
    "        \"\"\"\n",
    "        This step is a single deer iteration (will eventually be sequential scanned)\n",
    "        Args:\n",
    "          states: list of length num_layers of (T, D) shaped arrays\n",
    "          args: None\n",
    "        \"\"\"\n",
    "        states = [x] + states[:-1]  # length num_layers\n",
    "        print(f\"states shape is {states[0].shape}\")\n",
    "        fs = jnp.array(\n",
    "            jtu.tree_map(lambda x, f: f(x), states, layers)\n",
    "        )  # (num_layers, T,D) arrays, note that we keep states as a list so we can use jtu.tree_map\n",
    "        print(f\"fs shape is {fs.shape}\")\n",
    "        As = jnp.array(\n",
    "            jtu.tree_map(lambda x, f: jax.jacrev(f)(x), states, layers) # this line seems to be the bottleneck, but that's odd bc we'd expect someone to take this grads during backprop\n",
    "        )  # (num_layers, T, D, T, D) tensors\n",
    "        print(f\"As shape is {As.shape}\")\n",
    "        As = As.at[0].set(jnp.zeros((T, D, T, D)))\n",
    "        # pdb.set_trace()\n",
    "        # need to make the first A equal to zero\n",
    "        states = jnp.array(states)  # (num_layers, T,D)\n",
    "        # do some rearranging\n",
    "        print(\"starting the rearranges\")\n",
    "        flattened_states = jnp.reshape(states, (num_layers, T * D))  # (num_layers, T*D)\n",
    "        flattened_As = jnp.reshape(\n",
    "            As, (num_layers, T * D, T * D)\n",
    "        )  # (num_layers, T*D, T*D)\n",
    "        # somehow, we aren't even getting here\n",
    "        # print(\"we are about to start computing eigenvalues\")\n",
    "        # for A in flattened_As:\n",
    "        #     print(jnp.linalg.eigvals(A))\n",
    "        # pdb.set_trace()\n",
    "        flattened_fs = jnp.reshape(fs, (num_layers, T * D))  # (num_layers, T*D)\n",
    "        bs = flattened_fs - jnp.einsum(\n",
    "            \"tij,tj->ti\", flattened_As, flattened_states\n",
    "        )  # (num_layers, T*D)\n",
    "\n",
    "        # finally ready to evaluate linearized dynamics (in parallel)\n",
    "        print(\"we are about to start the associative scan\")\n",
    "        _, new_states = jax.lax.associative_scan(\n",
    "            binary_op, (flattened_As, bs)\n",
    "        )  # parallel operation\n",
    "        # new_states = jnp.nan_to_num(new_states)  # zero out nans, (num_layers, T*D)\n",
    "        new_states = jnp.reshape(new_states, (num_layers, T, D))  # (num_layers, T, D)\n",
    "        return list(new_states), new_states\n",
    "\n",
    "    print(\"starting deer outer loop\")\n",
    "    # states_guess, iter_hist = jax.lax.scan(\n",
    "    #     step, states_guess, None, length=num_iters\n",
    "    # )  # state_iters will show all the intermediate traces\n",
    "\n",
    "    iter_hist = []\n",
    "    for i in range(num_iters):\n",
    "        print()\n",
    "        print(\"-----------------\")\n",
    "        print(f\"iteration {i}\")\n",
    "        print(\"-----------------\")\n",
    "        print()\n",
    "        states_guess, iter_hist_add = step(states_guess, None)\n",
    "        iter_hist.append(iter_hist_add)\n",
    "\n",
    "    # return states_guess\n",
    "    return jnp.array(iter_hist) # (num_iters, num_layers, T, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5a7c63f-4be6-4bad-b6f5-b77c77bcc56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelArgs(NamedTuple):\n",
    "    dim: int\n",
    "    n_layers: int\n",
    "    n_heads: int\n",
    "    n_kv_heads: int\n",
    "    head_dim: int\n",
    "    hidden_dim: int\n",
    "    vocab_size: int\n",
    "    sliding_window: int\n",
    "    norm_eps: float\n",
    "    max_batch_size: int = 1\n",
    "\n",
    "# maybe we could do a proto_params.json, and just not load in the weights\n",
    "with open(\n",
    "    \"/Users/xaviergonzalez/Desktop/xavier_folders/stanford/cs229s/mistral_jax/model_files/params.json\",\n",
    "    \"r\",\n",
    ") as f:\n",
    "    #with open('./mistral-7B-v0.1/params.json', 'r') as f:\n",
    "    args = ModelArgs(**json.loads(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16a2d2cf-40f6-48af-a70f-37847532e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def port_weights_from_torch(torch_weights, eqx_model):\n",
    "    def load_weights(path, leaf):\n",
    "        path_pieces = []\n",
    "        for path_elem in path:\n",
    "            if isinstance(path_elem, jax.tree_util.GetAttrKey):\n",
    "                 path_pieces.append(path_elem.name)\n",
    "            elif isinstance(path_elem, jax.tree_util.SequenceKey):\n",
    "                 path_pieces.append(str(path_elem.idx))\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported path type {type(path_elem)}\")\n",
    "\n",
    "        path_pieces = \".\".join(path_pieces)\n",
    "        \n",
    "        if \"weight\" in path_pieces:\n",
    "            weight = torch_weights[path_pieces]\n",
    "            weight = jnp.asarray(weight.float().numpy(), dtype=jnp.bfloat16)\n",
    "            assert weight.shape == leaf.shape\n",
    "            assert weight.dtype == leaf.dtype\n",
    "            return weight\n",
    "        else:\n",
    "            print(f\"Weights not ported for: {path_pieces}\")\n",
    "            return leaf\n",
    "\n",
    "    return jax.tree_util.tree_map_with_path(load_weights, eqx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67530c78-98a9-4f84-aa4b-af0819b5f5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights not ported for: layers.0.dim\n",
      "Weights not ported for: layers.0.n_heads\n",
      "Weights not ported for: layers.0.attention.dim\n",
      "Weights not ported for: layers.0.attention.n_heads\n",
      "Weights not ported for: layers.0.attention.head_dim\n",
      "Weights not ported for: layers.0.attention.n_kv_heads\n",
      "Weights not ported for: layers.0.attention.kv_repeats\n",
      "Weights not ported for: layers.0.attention.sliding_window\n",
      "Weights not ported for: layers.0.attention.scale\n",
      "Weights not ported for: layers.0.attention_norm.eps\n",
      "Weights not ported for: layers.0.ffn_norm.eps\n",
      "Weights not ported for: layers.1.dim\n",
      "Weights not ported for: layers.1.n_heads\n",
      "Weights not ported for: layers.1.attention.dim\n",
      "Weights not ported for: layers.1.attention.n_heads\n",
      "Weights not ported for: layers.1.attention.head_dim\n",
      "Weights not ported for: layers.1.attention.n_kv_heads\n",
      "Weights not ported for: layers.1.attention.kv_repeats\n",
      "Weights not ported for: layers.1.attention.sliding_window\n",
      "Weights not ported for: layers.1.attention.scale\n",
      "Weights not ported for: layers.1.attention_norm.eps\n",
      "Weights not ported for: layers.1.ffn_norm.eps\n",
      "Weights not ported for: layers.2.dim\n",
      "Weights not ported for: layers.2.n_heads\n",
      "Weights not ported for: layers.2.attention.dim\n",
      "Weights not ported for: layers.2.attention.n_heads\n",
      "Weights not ported for: layers.2.attention.head_dim\n",
      "Weights not ported for: layers.2.attention.n_kv_heads\n",
      "Weights not ported for: layers.2.attention.kv_repeats\n",
      "Weights not ported for: layers.2.attention.sliding_window\n",
      "Weights not ported for: layers.2.attention.scale\n",
      "Weights not ported for: layers.2.attention_norm.eps\n",
      "Weights not ported for: layers.2.ffn_norm.eps\n",
      "Weights not ported for: layers.3.dim\n",
      "Weights not ported for: layers.3.n_heads\n",
      "Weights not ported for: layers.3.attention.dim\n",
      "Weights not ported for: layers.3.attention.n_heads\n",
      "Weights not ported for: layers.3.attention.head_dim\n",
      "Weights not ported for: layers.3.attention.n_kv_heads\n",
      "Weights not ported for: layers.3.attention.kv_repeats\n",
      "Weights not ported for: layers.3.attention.sliding_window\n",
      "Weights not ported for: layers.3.attention.scale\n",
      "Weights not ported for: layers.3.attention_norm.eps\n",
      "Weights not ported for: layers.3.ffn_norm.eps\n",
      "Weights not ported for: layers.4.dim\n",
      "Weights not ported for: layers.4.n_heads\n",
      "Weights not ported for: layers.4.attention.dim\n",
      "Weights not ported for: layers.4.attention.n_heads\n",
      "Weights not ported for: layers.4.attention.head_dim\n",
      "Weights not ported for: layers.4.attention.n_kv_heads\n",
      "Weights not ported for: layers.4.attention.kv_repeats\n",
      "Weights not ported for: layers.4.attention.sliding_window\n",
      "Weights not ported for: layers.4.attention.scale\n",
      "Weights not ported for: layers.4.attention_norm.eps\n",
      "Weights not ported for: layers.4.ffn_norm.eps\n",
      "Weights not ported for: layers.5.dim\n",
      "Weights not ported for: layers.5.n_heads\n",
      "Weights not ported for: layers.5.attention.dim\n",
      "Weights not ported for: layers.5.attention.n_heads\n",
      "Weights not ported for: layers.5.attention.head_dim\n",
      "Weights not ported for: layers.5.attention.n_kv_heads\n",
      "Weights not ported for: layers.5.attention.kv_repeats\n",
      "Weights not ported for: layers.5.attention.sliding_window\n",
      "Weights not ported for: layers.5.attention.scale\n",
      "Weights not ported for: layers.5.attention_norm.eps\n",
      "Weights not ported for: layers.5.ffn_norm.eps\n",
      "Weights not ported for: layers.6.dim\n",
      "Weights not ported for: layers.6.n_heads\n",
      "Weights not ported for: layers.6.attention.dim\n",
      "Weights not ported for: layers.6.attention.n_heads\n",
      "Weights not ported for: layers.6.attention.head_dim\n",
      "Weights not ported for: layers.6.attention.n_kv_heads\n",
      "Weights not ported for: layers.6.attention.kv_repeats\n",
      "Weights not ported for: layers.6.attention.sliding_window\n",
      "Weights not ported for: layers.6.attention.scale\n",
      "Weights not ported for: layers.6.attention_norm.eps\n",
      "Weights not ported for: layers.6.ffn_norm.eps\n",
      "Weights not ported for: layers.7.dim\n",
      "Weights not ported for: layers.7.n_heads\n",
      "Weights not ported for: layers.7.attention.dim\n",
      "Weights not ported for: layers.7.attention.n_heads\n",
      "Weights not ported for: layers.7.attention.head_dim\n",
      "Weights not ported for: layers.7.attention.n_kv_heads\n",
      "Weights not ported for: layers.7.attention.kv_repeats\n",
      "Weights not ported for: layers.7.attention.sliding_window\n",
      "Weights not ported for: layers.7.attention.scale\n",
      "Weights not ported for: layers.7.attention_norm.eps\n",
      "Weights not ported for: layers.7.ffn_norm.eps\n",
      "Weights not ported for: layers.8.dim\n",
      "Weights not ported for: layers.8.n_heads\n",
      "Weights not ported for: layers.8.attention.dim\n",
      "Weights not ported for: layers.8.attention.n_heads\n",
      "Weights not ported for: layers.8.attention.head_dim\n",
      "Weights not ported for: layers.8.attention.n_kv_heads\n",
      "Weights not ported for: layers.8.attention.kv_repeats\n",
      "Weights not ported for: layers.8.attention.sliding_window\n",
      "Weights not ported for: layers.8.attention.scale\n",
      "Weights not ported for: layers.8.attention_norm.eps\n",
      "Weights not ported for: layers.8.ffn_norm.eps\n",
      "Weights not ported for: layers.9.dim\n",
      "Weights not ported for: layers.9.n_heads\n",
      "Weights not ported for: layers.9.attention.dim\n",
      "Weights not ported for: layers.9.attention.n_heads\n",
      "Weights not ported for: layers.9.attention.head_dim\n",
      "Weights not ported for: layers.9.attention.n_kv_heads\n",
      "Weights not ported for: layers.9.attention.kv_repeats\n",
      "Weights not ported for: layers.9.attention.sliding_window\n",
      "Weights not ported for: layers.9.attention.scale\n",
      "Weights not ported for: layers.9.attention_norm.eps\n",
      "Weights not ported for: layers.9.ffn_norm.eps\n",
      "Weights not ported for: layers.10.dim\n",
      "Weights not ported for: layers.10.n_heads\n",
      "Weights not ported for: layers.10.attention.dim\n",
      "Weights not ported for: layers.10.attention.n_heads\n",
      "Weights not ported for: layers.10.attention.head_dim\n",
      "Weights not ported for: layers.10.attention.n_kv_heads\n",
      "Weights not ported for: layers.10.attention.kv_repeats\n",
      "Weights not ported for: layers.10.attention.sliding_window\n",
      "Weights not ported for: layers.10.attention.scale\n",
      "Weights not ported for: layers.10.attention_norm.eps\n",
      "Weights not ported for: layers.10.ffn_norm.eps\n",
      "Weights not ported for: layers.11.dim\n",
      "Weights not ported for: layers.11.n_heads\n",
      "Weights not ported for: layers.11.attention.dim\n",
      "Weights not ported for: layers.11.attention.n_heads\n",
      "Weights not ported for: layers.11.attention.head_dim\n",
      "Weights not ported for: layers.11.attention.n_kv_heads\n",
      "Weights not ported for: layers.11.attention.kv_repeats\n",
      "Weights not ported for: layers.11.attention.sliding_window\n",
      "Weights not ported for: layers.11.attention.scale\n",
      "Weights not ported for: layers.11.attention_norm.eps\n",
      "Weights not ported for: layers.11.ffn_norm.eps\n",
      "Weights not ported for: layers.12.dim\n",
      "Weights not ported for: layers.12.n_heads\n",
      "Weights not ported for: layers.12.attention.dim\n",
      "Weights not ported for: layers.12.attention.n_heads\n",
      "Weights not ported for: layers.12.attention.head_dim\n",
      "Weights not ported for: layers.12.attention.n_kv_heads\n",
      "Weights not ported for: layers.12.attention.kv_repeats\n",
      "Weights not ported for: layers.12.attention.sliding_window\n",
      "Weights not ported for: layers.12.attention.scale\n",
      "Weights not ported for: layers.12.attention_norm.eps\n",
      "Weights not ported for: layers.12.ffn_norm.eps\n",
      "Weights not ported for: layers.13.dim\n",
      "Weights not ported for: layers.13.n_heads\n",
      "Weights not ported for: layers.13.attention.dim\n",
      "Weights not ported for: layers.13.attention.n_heads\n",
      "Weights not ported for: layers.13.attention.head_dim\n",
      "Weights not ported for: layers.13.attention.n_kv_heads\n",
      "Weights not ported for: layers.13.attention.kv_repeats\n",
      "Weights not ported for: layers.13.attention.sliding_window\n",
      "Weights not ported for: layers.13.attention.scale\n",
      "Weights not ported for: layers.13.attention_norm.eps\n",
      "Weights not ported for: layers.13.ffn_norm.eps\n",
      "Weights not ported for: layers.14.dim\n",
      "Weights not ported for: layers.14.n_heads\n",
      "Weights not ported for: layers.14.attention.dim\n",
      "Weights not ported for: layers.14.attention.n_heads\n",
      "Weights not ported for: layers.14.attention.head_dim\n",
      "Weights not ported for: layers.14.attention.n_kv_heads\n",
      "Weights not ported for: layers.14.attention.kv_repeats\n",
      "Weights not ported for: layers.14.attention.sliding_window\n",
      "Weights not ported for: layers.14.attention.scale\n",
      "Weights not ported for: layers.14.attention_norm.eps\n",
      "Weights not ported for: layers.14.ffn_norm.eps\n",
      "Weights not ported for: layers.15.dim\n",
      "Weights not ported for: layers.15.n_heads\n",
      "Weights not ported for: layers.15.attention.dim\n",
      "Weights not ported for: layers.15.attention.n_heads\n",
      "Weights not ported for: layers.15.attention.head_dim\n",
      "Weights not ported for: layers.15.attention.n_kv_heads\n",
      "Weights not ported for: layers.15.attention.kv_repeats\n",
      "Weights not ported for: layers.15.attention.sliding_window\n",
      "Weights not ported for: layers.15.attention.scale\n",
      "Weights not ported for: layers.15.attention_norm.eps\n",
      "Weights not ported for: layers.15.ffn_norm.eps\n",
      "Weights not ported for: layers.16.dim\n",
      "Weights not ported for: layers.16.n_heads\n",
      "Weights not ported for: layers.16.attention.dim\n",
      "Weights not ported for: layers.16.attention.n_heads\n",
      "Weights not ported for: layers.16.attention.head_dim\n",
      "Weights not ported for: layers.16.attention.n_kv_heads\n",
      "Weights not ported for: layers.16.attention.kv_repeats\n",
      "Weights not ported for: layers.16.attention.sliding_window\n",
      "Weights not ported for: layers.16.attention.scale\n",
      "Weights not ported for: layers.16.attention_norm.eps\n",
      "Weights not ported for: layers.16.ffn_norm.eps\n",
      "Weights not ported for: layers.17.dim\n",
      "Weights not ported for: layers.17.n_heads\n",
      "Weights not ported for: layers.17.attention.dim\n",
      "Weights not ported for: layers.17.attention.n_heads\n",
      "Weights not ported for: layers.17.attention.head_dim\n",
      "Weights not ported for: layers.17.attention.n_kv_heads\n",
      "Weights not ported for: layers.17.attention.kv_repeats\n",
      "Weights not ported for: layers.17.attention.sliding_window\n",
      "Weights not ported for: layers.17.attention.scale\n",
      "Weights not ported for: layers.17.attention_norm.eps\n",
      "Weights not ported for: layers.17.ffn_norm.eps\n",
      "Weights not ported for: layers.18.dim\n",
      "Weights not ported for: layers.18.n_heads\n",
      "Weights not ported for: layers.18.attention.dim\n",
      "Weights not ported for: layers.18.attention.n_heads\n",
      "Weights not ported for: layers.18.attention.head_dim\n",
      "Weights not ported for: layers.18.attention.n_kv_heads\n",
      "Weights not ported for: layers.18.attention.kv_repeats\n",
      "Weights not ported for: layers.18.attention.sliding_window\n",
      "Weights not ported for: layers.18.attention.scale\n",
      "Weights not ported for: layers.18.attention_norm.eps\n",
      "Weights not ported for: layers.18.ffn_norm.eps\n",
      "Weights not ported for: layers.19.dim\n",
      "Weights not ported for: layers.19.n_heads\n",
      "Weights not ported for: layers.19.attention.dim\n",
      "Weights not ported for: layers.19.attention.n_heads\n",
      "Weights not ported for: layers.19.attention.head_dim\n",
      "Weights not ported for: layers.19.attention.n_kv_heads\n",
      "Weights not ported for: layers.19.attention.kv_repeats\n",
      "Weights not ported for: layers.19.attention.sliding_window\n",
      "Weights not ported for: layers.19.attention.scale\n",
      "Weights not ported for: layers.19.attention_norm.eps\n",
      "Weights not ported for: layers.19.ffn_norm.eps\n",
      "Weights not ported for: layers.20.dim\n",
      "Weights not ported for: layers.20.n_heads\n",
      "Weights not ported for: layers.20.attention.dim\n",
      "Weights not ported for: layers.20.attention.n_heads\n",
      "Weights not ported for: layers.20.attention.head_dim\n",
      "Weights not ported for: layers.20.attention.n_kv_heads\n",
      "Weights not ported for: layers.20.attention.kv_repeats\n",
      "Weights not ported for: layers.20.attention.sliding_window\n",
      "Weights not ported for: layers.20.attention.scale\n",
      "Weights not ported for: layers.20.attention_norm.eps\n",
      "Weights not ported for: layers.20.ffn_norm.eps\n",
      "Weights not ported for: layers.21.dim\n",
      "Weights not ported for: layers.21.n_heads\n",
      "Weights not ported for: layers.21.attention.dim\n",
      "Weights not ported for: layers.21.attention.n_heads\n",
      "Weights not ported for: layers.21.attention.head_dim\n",
      "Weights not ported for: layers.21.attention.n_kv_heads\n",
      "Weights not ported for: layers.21.attention.kv_repeats\n",
      "Weights not ported for: layers.21.attention.sliding_window\n",
      "Weights not ported for: layers.21.attention.scale\n",
      "Weights not ported for: layers.21.attention_norm.eps\n",
      "Weights not ported for: layers.21.ffn_norm.eps\n",
      "Weights not ported for: layers.22.dim\n",
      "Weights not ported for: layers.22.n_heads\n",
      "Weights not ported for: layers.22.attention.dim\n",
      "Weights not ported for: layers.22.attention.n_heads\n",
      "Weights not ported for: layers.22.attention.head_dim\n",
      "Weights not ported for: layers.22.attention.n_kv_heads\n",
      "Weights not ported for: layers.22.attention.kv_repeats\n",
      "Weights not ported for: layers.22.attention.sliding_window\n",
      "Weights not ported for: layers.22.attention.scale\n",
      "Weights not ported for: layers.22.attention_norm.eps\n",
      "Weights not ported for: layers.22.ffn_norm.eps\n",
      "Weights not ported for: layers.23.dim\n",
      "Weights not ported for: layers.23.n_heads\n",
      "Weights not ported for: layers.23.attention.dim\n",
      "Weights not ported for: layers.23.attention.n_heads\n",
      "Weights not ported for: layers.23.attention.head_dim\n",
      "Weights not ported for: layers.23.attention.n_kv_heads\n",
      "Weights not ported for: layers.23.attention.kv_repeats\n",
      "Weights not ported for: layers.23.attention.sliding_window\n",
      "Weights not ported for: layers.23.attention.scale\n",
      "Weights not ported for: layers.23.attention_norm.eps\n",
      "Weights not ported for: layers.23.ffn_norm.eps\n",
      "Weights not ported for: layers.24.dim\n",
      "Weights not ported for: layers.24.n_heads\n",
      "Weights not ported for: layers.24.attention.dim\n",
      "Weights not ported for: layers.24.attention.n_heads\n",
      "Weights not ported for: layers.24.attention.head_dim\n",
      "Weights not ported for: layers.24.attention.n_kv_heads\n",
      "Weights not ported for: layers.24.attention.kv_repeats\n",
      "Weights not ported for: layers.24.attention.sliding_window\n",
      "Weights not ported for: layers.24.attention.scale\n",
      "Weights not ported for: layers.24.attention_norm.eps\n",
      "Weights not ported for: layers.24.ffn_norm.eps\n",
      "Weights not ported for: layers.25.dim\n",
      "Weights not ported for: layers.25.n_heads\n",
      "Weights not ported for: layers.25.attention.dim\n",
      "Weights not ported for: layers.25.attention.n_heads\n",
      "Weights not ported for: layers.25.attention.head_dim\n",
      "Weights not ported for: layers.25.attention.n_kv_heads\n",
      "Weights not ported for: layers.25.attention.kv_repeats\n",
      "Weights not ported for: layers.25.attention.sliding_window\n",
      "Weights not ported for: layers.25.attention.scale\n",
      "Weights not ported for: layers.25.attention_norm.eps\n",
      "Weights not ported for: layers.25.ffn_norm.eps\n",
      "Weights not ported for: layers.26.dim\n",
      "Weights not ported for: layers.26.n_heads\n",
      "Weights not ported for: layers.26.attention.dim\n",
      "Weights not ported for: layers.26.attention.n_heads\n",
      "Weights not ported for: layers.26.attention.head_dim\n",
      "Weights not ported for: layers.26.attention.n_kv_heads\n",
      "Weights not ported for: layers.26.attention.kv_repeats\n",
      "Weights not ported for: layers.26.attention.sliding_window\n",
      "Weights not ported for: layers.26.attention.scale\n",
      "Weights not ported for: layers.26.attention_norm.eps\n",
      "Weights not ported for: layers.26.ffn_norm.eps\n",
      "Weights not ported for: layers.27.dim\n",
      "Weights not ported for: layers.27.n_heads\n",
      "Weights not ported for: layers.27.attention.dim\n",
      "Weights not ported for: layers.27.attention.n_heads\n",
      "Weights not ported for: layers.27.attention.head_dim\n",
      "Weights not ported for: layers.27.attention.n_kv_heads\n",
      "Weights not ported for: layers.27.attention.kv_repeats\n",
      "Weights not ported for: layers.27.attention.sliding_window\n",
      "Weights not ported for: layers.27.attention.scale\n",
      "Weights not ported for: layers.27.attention_norm.eps\n",
      "Weights not ported for: layers.27.ffn_norm.eps\n",
      "Weights not ported for: layers.28.dim\n",
      "Weights not ported for: layers.28.n_heads\n",
      "Weights not ported for: layers.28.attention.dim\n",
      "Weights not ported for: layers.28.attention.n_heads\n",
      "Weights not ported for: layers.28.attention.head_dim\n",
      "Weights not ported for: layers.28.attention.n_kv_heads\n",
      "Weights not ported for: layers.28.attention.kv_repeats\n",
      "Weights not ported for: layers.28.attention.sliding_window\n",
      "Weights not ported for: layers.28.attention.scale\n",
      "Weights not ported for: layers.28.attention_norm.eps\n",
      "Weights not ported for: layers.28.ffn_norm.eps\n",
      "Weights not ported for: layers.29.dim\n",
      "Weights not ported for: layers.29.n_heads\n",
      "Weights not ported for: layers.29.attention.dim\n",
      "Weights not ported for: layers.29.attention.n_heads\n",
      "Weights not ported for: layers.29.attention.head_dim\n",
      "Weights not ported for: layers.29.attention.n_kv_heads\n",
      "Weights not ported for: layers.29.attention.kv_repeats\n",
      "Weights not ported for: layers.29.attention.sliding_window\n",
      "Weights not ported for: layers.29.attention.scale\n",
      "Weights not ported for: layers.29.attention_norm.eps\n",
      "Weights not ported for: layers.29.ffn_norm.eps\n",
      "Weights not ported for: layers.30.dim\n",
      "Weights not ported for: layers.30.n_heads\n",
      "Weights not ported for: layers.30.attention.dim\n",
      "Weights not ported for: layers.30.attention.n_heads\n",
      "Weights not ported for: layers.30.attention.head_dim\n",
      "Weights not ported for: layers.30.attention.n_kv_heads\n",
      "Weights not ported for: layers.30.attention.kv_repeats\n",
      "Weights not ported for: layers.30.attention.sliding_window\n",
      "Weights not ported for: layers.30.attention.scale\n",
      "Weights not ported for: layers.30.attention_norm.eps\n",
      "Weights not ported for: layers.30.ffn_norm.eps\n",
      "Weights not ported for: layers.31.dim\n",
      "Weights not ported for: layers.31.n_heads\n",
      "Weights not ported for: layers.31.attention.dim\n",
      "Weights not ported for: layers.31.attention.n_heads\n",
      "Weights not ported for: layers.31.attention.head_dim\n",
      "Weights not ported for: layers.31.attention.n_kv_heads\n",
      "Weights not ported for: layers.31.attention.kv_repeats\n",
      "Weights not ported for: layers.31.attention.sliding_window\n",
      "Weights not ported for: layers.31.attention.scale\n",
      "Weights not ported for: layers.31.attention_norm.eps\n",
      "Weights not ported for: layers.31.ffn_norm.eps\n",
      "Weights not ported for: norm.eps\n",
      "Weights not ported for: vocab_size\n",
      "Weights not ported for: n_layers\n",
      "Weights not ported for: sliding_window\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(args, key=jax.random.PRNGKey(1), dtype=jnp.bfloat16) # sets architecutre\n",
    "model = port_weights_from_torch(state_dict, model) # fills with pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7da969f6-eb91-4df5-9976-1db480914a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache_k = jnp.zeros((args.max_batch_size, args.n_layers, args.sliding_window, args.n_kv_heads, args.head_dim), dtype=jnp.bfloat16)\n",
    "# cache_v = jnp.zeros((args.max_batch_size, args.n_layers, args.sliding_window, args.n_kv_heads, args.head_dim), dtype=jnp.bfloat16)\n",
    "NUM_ITERS = 15\n",
    "cos_freq, sin_freq = precompute_frequencies(args.head_dim, 128000)\n",
    "# vmapped = jax.vmap(partial(model.parallel_call, num_iters=NUM_ITERS), in_axes=(0, None, None, None, None)) # vmapped is the name of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmap_seq = jax.vmap(\n",
    "    model,\n",
    "    in_axes=(0, None, None, None, None),\n",
    ")  # vmapped is the name of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "001d96e5-f58d-4ea4-b878-92abcfcb85e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_pos = jnp.array([0], dtype=jnp.int32)\n",
    "fake_inp = jnp.asarray([[1]], dtype=jnp.int32)\n",
    "fake_mask = None\n",
    "\n",
    "hist_seq = vmap_seq(\n",
    "    fake_inp, cos_freq[fake_pos], sin_freq[fake_pos], fake_pos, fake_mask\n",
    ")\n",
    "# hist_parr = vmapped(fake_inp, cos_freq[fake_pos], sin_freq[fake_pos], fake_pos, fake_mask)\n",
    "jnp.save(f\"seq_history.npy\", hist_seq)\n",
    "# jnp.save(f\"parallel_history_NumIters_{NUM_ITERS}.npy\", hist_parr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_parr = jnp.load(\"parallel_history_NumIters_15.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 32)\n"
     ]
    }
   ],
   "source": [
    "errors_per_iter_and_layer = jnp.mean(jnp.abs((hist_parr[0] - hist_seq).squeeze()), axis=-1)\n",
    "print(jnp.shape(errors_per_iter_and_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x67719fc20>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(errors_per_iter_and_layer[5], label=\"iteration 5\")\n",
    "plt.plot(errors_per_iter_and_layer[10], label=\"iteration 10\")\n",
    "plt.xlabel(\"layers\")\n",
    "plt.ylabel(\"mean error in activations\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x678f72810>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "plt.imshow(errors_per_iter_and_layer[2:], cmap=\"hot\",norm=mcolors.LogNorm(), interpolation=\"nearest\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Partial(\n",
       "  func=_JitWrapper(\n",
       "    fn='Transformer.compute_norm',\n",
       "    filter_warning=False,\n",
       "    donate_first=False,\n",
       "    donate_rest=False\n",
       "  ),\n",
       "  args=(\n",
       "    Transformer(\n",
       "      tok_embeddings=Embedding(\n",
       "        num_embeddings=32000,\n",
       "        embedding_size=4096,\n",
       "        weight=bf16[32000,4096]\n",
       "      ),\n",
       "      layers=[\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        ),\n",
       "        TransformerBlock(\n",
       "          dim=4096,\n",
       "          n_heads=32,\n",
       "          attention=Attention(\n",
       "            dim=4096,\n",
       "            n_heads=32,\n",
       "            head_dim=128,\n",
       "            n_kv_heads=8,\n",
       "            kv_repeats=4,\n",
       "            sliding_window=4096,\n",
       "            scale=0.08838834764831845,\n",
       "            wq=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wk=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wv=Linear(\n",
       "              weight=bf16[1024,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=1024,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            wo=Linear(\n",
       "              weight=bf16[4096,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          attention_norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "          feed_forward=FeedForward(\n",
       "            w1=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w2=Linear(\n",
       "              weight=bf16[4096,14336],\n",
       "              bias=None,\n",
       "              in_features=14336,\n",
       "              out_features=4096,\n",
       "              use_bias=False\n",
       "            ),\n",
       "            w3=Linear(\n",
       "              weight=bf16[14336,4096],\n",
       "              bias=None,\n",
       "              in_features=4096,\n",
       "              out_features=14336,\n",
       "              use_bias=False\n",
       "            )\n",
       "          ),\n",
       "          ffn_norm=RMSNorm(eps=1e-05, weight=bf16[4096])\n",
       "        )\n",
       "      ],\n",
       "      norm=RMSNorm(eps=1e-05, weight=bf16[4096]),\n",
       "      output=Linear(\n",
       "        weight=bf16[32000,4096],\n",
       "        bias=None,\n",
       "        in_features=4096,\n",
       "        out_features=32000,\n",
       "        use_bias=False\n",
       "      ),\n",
       "      vocab_size=32000,\n",
       "      n_layers=32,\n",
       "      sliding_window=4096\n",
       "    ),\n",
       "  ),\n",
       "  keywords={}\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compute_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[[[-8.92639160e-04,  2.65502930e-03, -1.84570312e-01, ...,\n",
       "            7.62939453e-06, -1.43432617e-03, -1.95312500e-03]],\n",
       "\n",
       "         [[-8.91094469e-03, -6.18494023e-03, -1.85622007e-01, ...,\n",
       "            9.09190509e-04, -5.82782552e-03, -1.42973997e-02]],\n",
       "\n",
       "         [[-6.07582554e-03, -3.72581817e-02, -1.96056485e-01, ...,\n",
       "           -2.17162538e-02, -3.14874249e-03, -9.00264457e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.10669410e+00,  1.49299145e-01, -5.95834732e-01, ...,\n",
       "            3.55023921e-01,  1.29173613e+00, -5.56365788e-01]],\n",
       "\n",
       "         [[ 1.05325460e+00,  1.63162947e-02, -5.39088607e-01, ...,\n",
       "            5.94611347e-01,  1.15215635e+00, -6.52567387e-01]],\n",
       "\n",
       "         [[ 4.23761547e-01,  2.28124112e-01, -1.06671560e+00, ...,\n",
       "            5.91840267e-01,  8.96652579e-01, -7.07928896e-01]]],\n",
       "\n",
       "\n",
       "        [[[-8.92639160e-04,  2.65502930e-03, -1.84570312e-01, ...,\n",
       "            7.62939453e-06, -1.43432617e-03, -1.95312500e-03]],\n",
       "\n",
       "         [[-1.20094955e-01, -3.91652256e-01, -1.39715612e-01, ...,\n",
       "            7.06485100e-03,  2.07763314e-02, -5.91178797e-02]],\n",
       "\n",
       "         [[-1.16284236e-01, -4.00582671e-01, -2.51705170e-01, ...,\n",
       "            2.94541754e-02,  1.13582872e-02, -9.83709916e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 6.72007227e+00,  1.60713234e+01, -9.77484894e+00, ...,\n",
       "           -4.83511925e+00,  2.95893650e+01, -2.57335949e+00]],\n",
       "\n",
       "         [[ 9.25865936e+00,  1.67624340e+01, -5.88470030e+00, ...,\n",
       "            4.59273815e-01,  3.09054546e+01, -3.87069154e+00]],\n",
       "\n",
       "         [[ 1.20738783e+01,  1.76835766e+01, -8.13421631e+00, ...,\n",
       "            2.23497677e+00,  3.07850189e+01, -5.26595545e+00]]]]],      dtype=float32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "75b07b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_inp_flat = jnp.asarray([1, 832, 349, 265, 1369], dtype=jnp.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "07a00741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[-0.000892639, 0.00265503, -0.18457, ..., 7.62939e-06,\n",
       "         -0.00143433, -0.00195312],\n",
       "        [0.00537109, 0.0018158, 0.00552368, ..., 0.00354004,\n",
       "         -0.000892639, 0.000926971],\n",
       "        [0.00579834, 0.00128937, 0.00546265, ..., 0.00196838,\n",
       "         -0.00120544, -0.00204468],\n",
       "        [0.00314331, 0.000362396, 0.00439453, ..., 0.000984192,\n",
       "         0.00153351, 0.00172424],\n",
       "        [0.00219727, 0.00561523, 0.00280762, ..., 0.00230408,\n",
       "         0.00222778, -0.000953674]],\n",
       "\n",
       "       [[-0.120117, -0.392578, -0.139648, ..., 0.00674438, 0.020752,\n",
       "         -0.059082],\n",
       "        [0.00115967, -0.00131226, 0.00242615, ..., 0.0114136,\n",
       "         -0.00276184, -0.000175476],\n",
       "        [0.00506592, 0.00265503, 0.00567627, ..., 0.00622559,\n",
       "         -0.0065918, -0.0043335],\n",
       "        [0.000976562, -0.00344849, 0.0111694, ..., 0.00891113,\n",
       "         -0.00165558, -9.15527e-05],\n",
       "        [0.00546265, 0.00460815, 0.00469971, ..., -0.000923157,\n",
       "         -0.000442505, -0.0078125]],\n",
       "\n",
       "       [[-0.121582, -0.392578, -0.140625, ..., 0.00622559, 0.0209961,\n",
       "         -0.0600586],\n",
       "        [-0.000591278, -0.00970459, 0.00531006, ..., 0.0177002,\n",
       "         0.00062561, -0.00817871],\n",
       "        [0.0065918, 7.62939e-05, 0.00305176, ..., 0.0136719, -0.0135498,\n",
       "         -0.00128174],\n",
       "        [-0.000236511, -3.05176e-05, 0.0106812, ..., 0.019043,\n",
       "         -0.00854492, -0.00171661],\n",
       "        [0.00192261, -0.0020752, 0.00750732, ..., 0.00386047,\n",
       "         -0.00454712, -0.00817871]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.527344, -0.570312, -0.314453, ..., -0.578125, -0.40625,\n",
       "         -0.090332],\n",
       "        [-1.04688, 0.00927734, 0.00500488, ..., 0.503906, -0.449219,\n",
       "         -0.0654297],\n",
       "        [0.015625, -0.0483398, 0.167969, ..., 0.34375, -0.15332,\n",
       "         0.416016],\n",
       "        [-0.186523, 0.0078125, 0.146484, ..., 0.0761719, -0.71875,\n",
       "         0.435547],\n",
       "        [-0.0263672, -0.0253906, 0.0388184, ..., -0.292969, -0.503906,\n",
       "         -0.0305176]],\n",
       "\n",
       "       [[-0.914062, -0.90625, -1.01562, ..., -1.11719, -0.617188,\n",
       "         0.151367],\n",
       "        [-1.07812, -0.308594, 0.0322266, ..., 0.644531, -0.458984,\n",
       "         -0.105957],\n",
       "        [0.166992, -0.0170898, 0.0146484, ..., 0.535156, -0.229492,\n",
       "         0.609375],\n",
       "        [-0.165039, -0.174805, 0.175781, ..., -0.192383, -0.707031,\n",
       "         0.388672],\n",
       "        [0.113281, 0.0927734, 0.166016, ..., -0.257812, -0.621094,\n",
       "         -0.105957]],\n",
       "\n",
       "       [[-1.60938, 0.472656, -1.625, ..., -0.0390625, -1.82031, 2.5625],\n",
       "        [-1.36719, 0.243164, -0.225586, ..., 0.75, -0.617188, -0.34375],\n",
       "        [0.0698242, 0.466797, -0.0432129, ..., 0.53125, -0.208984,\n",
       "         0.488281],\n",
       "        [0.207031, -0.0561523, 0.180664, ..., -0.11377, -0.408203,\n",
       "         0.455078],\n",
       "        [0.232422, 0.212891, 0.050293, ..., -0.34375, -0.695312,\n",
       "         -0.182617]]], dtype=bfloat16)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(fake_inp_flat, cos_freq[fake_pos], sin_freq[fake_pos], fake_pos, fake_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.RMSNorm"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.0936636, dtype=float32)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.sqrt(jnp.mean(logits[0, -1, 0] ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1a3b4f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(5.442458, dtype=float32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.sqrt(jnp.mean(model.norm(logits[0, -1, 0]) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e7b45669",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_inp_flat = jnp.asarray([1, 832, 349, 265, 1369], dtype=jnp.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9d2ae933",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = Attention(args, key=jax.random.PRNGKey(1), dtype=jnp.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4ffc5c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128000, 64)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_freq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "89ac0483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4096)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compute_embeddings(fake_inp_flat).shape # (T, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc248cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.compute_embeddings(fake_inp_flat)\n",
    "xq, xk, xv = attn.compute_qkv(x)\n",
    "key, value = attn.prefill(xk, xv)\n",
    "output = attn.compute_scores_and_output(xq, key, value, fake_mask, x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406da556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4096)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e23a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4096)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949a4e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 8, 128)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a71fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 32, 128)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe1f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 32, 128)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1392db12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 32, 128)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e8e3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 32, 128)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq.shape # (T, n_heads, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe4f380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 8, 128)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xk.shape # (T, n_kv_heads, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167bed62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 8, 128)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xv.shape # (T, n_kv_heads, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5c6b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn.compute_scores_and_output(xq, xk, xv, fake_mask, 5).shape # (T, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900a9b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_pos.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5374a93",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'at'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_inp_flat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos_freq\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin_freq\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_pos\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 96\u001b[0m, in \u001b[0;36mAttention.__call__\u001b[0;34m(self, x, cos_freq, sin_freq, positions, mask, cache_k, cache_v)\u001b[0m\n\u001b[1;32m     93\u001b[0m xk \u001b[38;5;241m=\u001b[39m calculate_rope(xk, cos_freq, sin_freq, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# 3. Update cache\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m cache_k, cache_v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_cache_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# 4. Generation\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m positions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# prefill\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[8], line 53\u001b[0m, in \u001b[0;36mAttention.update_cache_values\u001b[0;34m(self, xk, xv, cache_k, cache_v, positions)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;129m@jax\u001b[39m\u001b[38;5;241m.\u001b[39mjit\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_cache_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, xk, xv, cache_k, cache_v, positions):\n\u001b[0;32m---> 53\u001b[0m     cache_k \u001b[38;5;241m=\u001b[39m \u001b[43mcache_k\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat\u001b[49m[positions, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\u001b[38;5;241m.\u001b[39mset(xk[positions, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m])\n\u001b[1;32m     54\u001b[0m     cache_v \u001b[38;5;241m=\u001b[39m cache_v\u001b[38;5;241m.\u001b[39mat[positions, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\u001b[38;5;241m.\u001b[39mset(xv[positions, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m])\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cache_k, cache_v\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'at'"
     ]
    }
   ],
   "source": [
    "attn(model.compute_embeddings(fake_inp_flat), cos_freq[:5], sin_freq[:5], fake_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3410598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc44e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 4096)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.vmap(model.compute_embeddings)(fake_inp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c401172d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 32000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape # (batch, T, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a48e48",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`eqx.nn.Embedding()(x)` should be called with a scalar index `x`. Use `jax.vmap` if you would like to index with multiple values.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfake_inp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcos_freq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfake_pos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msin_freq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfake_pos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfake_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfake_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 53\u001b[0m, in \u001b[0;36mTransformer.__call__\u001b[0;34m(self, x, cos_freq, sin_freq, positions, mask, cache_k, cache_v)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, cos_freq, sin_freq, positions, mask, cache_k, cache_v):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# x is of shape (seqlen, )\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     56\u001b[0m         seqlen \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mistral-eqx/lib/python3.12/site-packages/equinox/_module.py:1189\u001b[0m, in \u001b[0;36mPartial.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the wrapped `self.func`.\u001b[39;00m\n\u001b[1;32m   1178\u001b[0m \n\u001b[1;32m   1179\u001b[0m \u001b[38;5;124;03m    **Arguments:**\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;124;03m    The result of the wrapped function.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeywords\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 16 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m, in \u001b[0;36mTransformer.compute_embeddings\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;129m@eqx\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_jit\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtok_embeddings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mistral-eqx/lib/python3.12/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mistral-eqx/lib/python3.12/site-packages/equinox/nn/_embedding.py:101\u001b[0m, in \u001b[0;36mEmbedding.__call__\u001b[0;34m(self, x, key)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight[x]\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`eqx.nn.Embedding()(x)` should be called with a scalar index `x`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `jax.vmap` if you would like to index with multiple values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: `eqx.nn.Embedding()(x)` should be called with a scalar index `x`. Use `jax.vmap` if you would like to index with multiple values."
     ]
    }
   ],
   "source": [
    "model(\n",
    "    fake_inp,\n",
    "    cos_freq[fake_pos],\n",
    "    sin_freq[fake_pos],\n",
    "    fake_pos,\n",
    "    fake_mask,\n",
    "    cache_k,\n",
    "    cache_v,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c5dec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/xaviergonzalez/opt/anaconda3/envs/mistral-eqx/lib/python3.12/site-packages/equinox/nn/_embedding.py\u001b[0m(101)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     99 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    100 \u001b[0;31m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 101 \u001b[0;31m            raise ValueError(\n",
      "\u001b[0m\u001b[0;32m    102 \u001b[0;31m                \u001b[0;34m\"`eqx.nn.Embedding()(x)` should be called with a scalar index `x`. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    103 \u001b[0;31m                \u001b[0;34m\"Use `jax.vmap` if you would like to index with multiple values.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/Users/xaviergonzalez/opt/anaconda3/envs/mistral-eqx/lib/python3.12/contextlib.py\u001b[0m(81)\u001b[0;36minner\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     79 \u001b[0;31m        \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     80 \u001b[0;31m            \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 81 \u001b[0;31m                \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     82 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     83 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/var/folders/tf/ybkfqmld4yb8_yn2xr11sl8m0000gn/T/ipykernel_2338/467368851.py\u001b[0m(24)\u001b[0;36mcompute_embeddings\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     22 \u001b[0;31m    \u001b[0;34m@\u001b[0m\u001b[0meqx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_jit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     23 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mcompute_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 24 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     25 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     26 \u001b[0;31m    \u001b[0;34m@\u001b[0m\u001b[0meqx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_jit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;31m    [... skipped 3 hidden frame(s)]\u001b[0m\n",
      "\n",
      "(1, 5)\n",
      "*** ValueError: `eqx.nn.Embedding()(x)` should be called with a scalar index `x`. Use `jax.vmap` if you would like to index with multiple values.\n",
      "*** ValueError: `eqx.nn.Embedding()(x)` should be called with a scalar index `x`. Use `jax.vmap` if you would like to index with multiple values.\n",
      "*** jax.errors.UnexpectedTracerError: Encountered an unexpected tracer. A function transformed by JAX had a side effect, allowing for a reference to an intermediate value with type int32[1,5] wrapped in a DynamicJaxprTracer to escape the scope of the transformation.\n",
      "JAX transformations require that functions explicitly return their outputs, and disallow saving intermediate values to global state.\n",
      "The function being traced when the value leaked was compute_embeddings at /Users/xaviergonzalez/opt/anaconda3/envs/mistral-eqx/lib/python3.12/site-packages/equinox/_jit.py:37 traced for jit.\n",
      "------------------------------\n",
      "The leaked intermediate value was created on line /var/folders/tf/ybkfqmld4yb8_yn2xr11sl8m0000gn/T/ipykernel_2338/467368851.py:53:12 (Transformer.__call__). \n",
      "------------------------------\n",
      "When the value was created, the final 5 stack frames (most recent last) excluding JAX-internal frames were:\n",
      "------------------------------\n",
      "<frozen runpy>:198:11 (_run_module_as_main)\n",
      "<frozen runpy>:88:4 (_run_code)\n",
      "/var/folders/tf/ybkfqmld4yb8_yn2xr11sl8m0000gn/T/ipykernel_2338/1221483641.py:1 (<module>)\n",
      "/var/folders/tf/ybkfqmld4yb8_yn2xr11sl8m0000gn/T/ipykernel_2338/467368851.py:53:12 (Transformer.__call__)\n",
      "------------------------------\n",
      "\n",
      "To catch the leak earlier, try setting the environment variable JAX_CHECK_TRACER_LEAKS or using the `jax.checking_leaks` context manager.\n",
      "See https://jax.readthedocs.io/en/latest/errors.html#jax.errors.UnexpectedTracerError\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e223d912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral-eqx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
